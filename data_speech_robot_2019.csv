"","scopusID","doi","pmid","authors","affiliations","countries","year","articletitle","journal","volume","issue","pages","keywords","abstract","ptype","timescited"
"1","2-s2.0-85059542433","10.1016/j.cogsys.2018.12.010",NA,"Mi J.|Tang S.|Deng Z.|Goerner M.|Zhang J.","Universität Hamburg","Germany","2019","Object affordance based multimodal fusion for natural Human-Robot interaction","Cognitive Systems Research","54",NA,"128-137","Multimodal fusion|Natural Human-Robot interaction|Object affordance recognition","© 2018 Elsevier B.V. Spoken language based natural Human-Robot Interaction (HRI) requires robots to have the ability to understand spoken language, and extract the intention-related information from the working scenario. For grounding the intention-related object in the working environment, object affordance recognition could be a feasible way. To this end, we propose a dataset and a deep CNN based architecture to learn the human-centered object affordance. Furthermore, we present an affordance based multimodal fusion framework to realize intended object grasping according to the spoken instructions of human users. The proposed framework contains an intention semantics extraction module which is employed to extract the intention from spoken language, a deep Convolutional Neural Network (CNN) based object affordance recognition module which is applied to recognize human-centered object affordance, and a multimodal fusion module which is adopted to bridge the extracted intentions and the recognized object affordances. We also complete multiple intended object grasping experiments on a PR2 platform to validate the feasibility and practicability of the presented HRI framework.","Article","0"
"2","2-s2.0-85057173888","10.1016/j.ijhcs.2018.11.009",NA,"Cerezo R.|Calderón V.|Romero C.","Universidad de Oviedo|Santiago Apóstol Infant and Primary School|Universidad de Cordoba","Spain","2019","A holographic mobile-based application for practicing pronunciation of basic English vocabulary for Spanish speaking children","International Journal of Human Computer Studies","124",NA,"13-25","English foreign language|Holograms|Interactive learning environments|Learning at early age|Mobile applications","© 2018 This paper describes a holographic mobile-based application designed to help Spanish-speaking children to practice the pronunciation of basic English vocabulary words. The mastery of vocabulary is a fundamental step when learning a language but is often perceived as boring. Producing the correct pronunciation is frequently regarded as the most difficult and complex skill for new learners of English. In order to address these problems this research takes advantage of the power of multi-channel stimuli (sound, image and interaction) in a mobile-based hologram application in order to motivate students and improve their experience of practicing. We adapted the prize-winning HolograFX game and developed a new mobile application to help practice English pronunciation. A 3D holographic robot that acts as a virtual teacher interacts via voice with the children. To test the tool we carried out an experiment with 70 Spanish pre-school children divided into three classes, the control group using traditional methods such as images in books and on the blackboard, and two experimental groups using our drills and practice software. One experimental group used the mobile application without the holographic game and the other experimental group used the application with the holographic game. We performed pre-test and post-test performance assessments, a satisfaction survey and emotion analysis. The results are very promising. They show that the use of the holographic mobile-based application had a significant impact on the children's motivation. It also improved their performance compared to traditional methods used in the classroom.","Article","0"
"3","2-s2.0-85060983455","10.1016/j.robot.2019.01.007",NA,"Savage J.|Rosenblueth D.A.|Matamoros M.|Negrete M.|Contreras L.|Cruz J.|Martell R.|Estrada H.|Okada H.","Universidad Nacional Autónoma de México|Universität Koblenz-Landau|Tamagawa University","Mexico|Germany|Japan","2019","Semantic reasoning in service robots using expert systems","Robotics and Autonomous Systems","114",NA,"77-92","Knowledge representation|Semantic reasoning|Service robots","© 2019 Elsevier B.V. This paper presents the semantic-reasoning module of VIRBOT, our proposed architecture for service robots. We show that by combining symbolic AI with digital-signal processing techniques this module achieves competitive performance. Our system translates a voice command into an unambiguous representation that helps an inference engine, built around an expert system, to perform action and motion planning. First, in the natural-language interpretation process, the system generates two outputs: (1) conceptual dependence, expressing the linguistic meaning of the statement, and (2) verbal confirmation, a paraphrase in natural language that is repeated to the user to confirm that the command has been correctly understood. Then, a conceptual-dependency interpreter extracts semantic role structures from the input sentence and looks for such structures in a set of known interpretation patterns. We evaluate this approach in a series of skill-specific semantic-reasoning experiments. Finally, we demonstrate our system in the general-purpose service robot test of the RoboCup-at-Home international competition, where incomplete information is given to a robot and the robot must recognize and request the missing information, and we compare our results with a series of baselines from the competition where our proposal performed best.","Article","0"
"4","2-s2.0-85013170693","10.1007/s00146-017-0700-0",NA,"Dewandre N.","European Commission","Belgium","2019","Humans as relational selves","AI and Society","34","1","95-98","Attention|Ethics|Hannah Arendt|Hyperconnectivity|Post-human|Rational subject|Relational self","© 2017, Springer-Verlag London. Instead of wondering about the nature of robots, as if our thinking about humans was stable and straightforward, we should dig deeper in thinking about how we think about humans. Indeed, the emotions embedded in the ethical approaches to robots and artificial intelligence, are rooted in a long tradition of thinking about humans, either in an instrumental or in a pseudo-divine way. Both perspectives miss humanness, and are misleading when it comes to thinking about robots and their relationships with humans. With the instrumental way to grasp humanness, humans are seen as machines and, by the same token, robots can easily be seen as human, as a matter of fact. With the quasi-divine way to grasp humanness, humans are seen as aspiring omniscient-omnipotent creatures and, by the same token, robots are projected to be, what men will always fail to become. Hence, our way to think about robots is mirroring our way to think about humans…as long as we hold rationality as a distinctive criteria for humanness. The text below flows from a TEDxULB talk that took place in Brussels on May 4, 2016 (https://www.youtube.com/watch?v=VcGywYSJlf0). It calls for leaving behind the rational subject as proxy for humanness, and embracing instead the figure of the relational self. The relational self is rooted in the Arendtian concept of plurality. Embracing the relational self, instead of the rational subject, has several advantages: it allows to distinguish humans from artefacts; it allows to grasp the dynamics between control, orientation, and recognition and to understand how human freedom flows from this dynamics; it opens to the foregrounding of vulnerability, as a shared characteristic of humanness, instead of as a defect touching only some; last, and surely not least, it points to a new form of vulnerability: that of our attentional spheres, whose protection may deserve a new fundamental right, in order to ensure our integrity, besides and beyond body and home.","Note","0"
"5","2-s2.0-85014052071","10.1007/s00146-017-0697-4",NA,"Shotter J.","University of New Hampshire Durham","United States","2019","Why being dialogical must come before being logical: the need for a hermeneutical–dialogical approach to robotic activities","AI and Society","34","1","29-35","Anticipations|Bakhtin|Bergson|Buber|Craik|Dewey|Dialogical|Dreyfus|Embodiment|Gadamer|Hermeneutics|Merleau-Ponty|Relationality|Social institutions|Wittgenstein","© 2017, Springer-Verlag London. Currently, our official rationality is still of a Cartesian kind; we are still embedded in a mechanistic order that takes it that separate, countable entities (spatial forms), related logically to each other, are the only ‘things’ that matter to us—an order clearly suited to advances in robotics. Unfortunately, it is an order that renders invisible ‘relational things’, non-objective things that exist in time, in the transitions from one state of affairs to another, things that ‘point’ toward possibilities in the future, which mean something to us. I have called such things, hermeneutical–dialogical ‘things’ as they gradually emerge in our back-and-forth, step-by-step relations to the others and otherneses in our surroundings; they consist in the ‘promissory’ things sustaining our trust in each other and in our authorities, in our social organizations and social institutions, and in our culture. Clearly, we need to understand better, not only what robots can, and cannot do, but also the long-term ethical and political implications of inserting robotic activities into our everyday ways of relating ourselves to our surroundings if we are to avoid the dystopian futures envisaged by some. Descartes’ aim of “making ourselves, as it were, masters and possessors of nature,” forgets our larger task of our making ourselves into human beings—of doing together in dialog what we cannot do apart.","Article","0"
"6","2-s2.0-85042213001","10.1007/s00146-018-0828-6",NA,"Wegerif R.|Major L.","University of Cambridge","United Kingdom","2019","Buber, educational technology, and the expansion of dialogic space","AI and Society","34","1","109-119","Buber|Dialogic space theory|Dialogue|Educational technology|Voice","© 2018, The Author(s). Buber’s distinction between the ‘I-It’ mode and the ‘I-Thou’ mode is seminal for dialogic education. While Buber introduces the idea of dialogic space, an idea which has proved useful for the analysis of dialogic education with technology, his account fails to engage adequately with the role of technology. This paper offers an introduction to the significance of the I-It/I-Thou duality of technology in relation with opening dialogic space. This is followed by a short schematic history of educational technology which reveals the role technology plays, not only in opening dialogic space, but also in expanding dialogic space. The expansion of dialogic space is an expansion of what it means to be ‘us’ as dialogic engagement facilitates the incorporation, into our shared sense of identity, of aspects of reality that are initially experienced as alien or ‘other’. Augmenting Buber with an alternative understanding of dialogic space enables us to see how dialogue mediated by technology, as well as dialogue with monologised fragments of technology (robots), can, through education, lead to an expansion of what it means to be human.","Article","3"
"7","2-s2.0-85053222334","10.1007/s00146-018-0859-z",NA,"Baniwal V.","University of Delhi","India","2019","Reconsidering Buber, educational technology, and the expansion of dialogic space","AI and Society","34","1","121-127","Buber|Dialogue|Dialogue via technology|Dialogue with technology|Intersubjectivity","© 2018, Springer-Verlag London Ltd., part of Springer Nature. This paper is an attempt to further the conversation about the possibilities of dialogue with technology that Wegerif and Major (AI Soc, https://doi.org/10.1007/s00146-018-0828-6, 2018) have initiated. In their paper Wegerif and Major (AI Soc, https://doi.org/10.1007/s00146-018-0828-6, 2018) have argued that “constructive dialogue with technology is possible, even essential, and that this takes the form of opening a dialogic space” and they also “argue against Buber that dialogic spaces do not all take the same form, but that they take a multitude of forms depending, to a large extent, on the mediating technology”. Reflecting on the paper by Wegerif and Major (AI Soc, https://doi.org/10.1007/s00146-018-0828-6, 2018), the present paper attempts to highlight certain issues and possibilities for such a dialogue. In the first section of the paper, Wegerif and Major’s (AI Soc, https://doi.org/10.1007/s00146-018-0828-6, 2018) discussion of technology-mediated dialogue has been reflected upon. This is followed by a reflection on the possibility and challenges of engaging in a dialogue with technology as ‘other’ by referring to the case of a robot ‘Sophia’ getting citizenship in the Kingdom of Saudi Arabia. The third section discusses certain concerns regarding the interpretation of Buber’s idea of dialogue by Wegerif and Major (AI Soc, https://doi.org/10.1007/s00146-018-0828-6, 2018). The conclusive section highlights some key aspects that need to be considered while reflecting on the possibility of dialogue with technology.","Article","0"
"8","2-s2.0-85057615336","10.1016/j.compedu.2018.11.008",NA,"Lei M.|Clemente I.M.|Hu Y.","Michigan State University","United States","2019","Student in the shell: The robotic body and student engagement","Computers and Education","130",NA,"59-80","Embodiment|Robots|Social presence|Student engagement|Synchronous learning","© 2018 Elsevier Ltd The purpose of this case study was to explore how the embodiment of graduate students in robotic surrogates was related to their engagement in a class with other robotically and non-robotically embodied classmates. Using a mixed methods design, we collected survey and in-class observational data on the students’ perceptions of their robotic bodies and their engagement in the course. Applying linear models and thematic analysis, we sought to identify prevailing patterns of students’ use their robotic bodies to engage in their learning. Our findings suggest that nonverbal communication with one's robotic body is a dominant form of interaction and engagement in synchronous learning contexts and multiple contextual factors affect robotic students’ engagement. Specifically, the capabilities of the robotic surrogate and the student's perceptions of the surrogate as an extension of their own body may influence their engagement in educational contexts. Patterns of robotic and nonverbal behaviors may also vary with instructional context and social learning. For robotically telepresent students, embodiment becomes a central factor to their engagement and should be included in theories of engagement in technology-mediated contexts that involve surrogate bodies.","Article","0"
"9","2-s2.0-85060187209","10.1016/j.ridd.2019.01.002","30677695","So W.C.|Wong M.K.Y.|Lam W.Y.|Cheng C.H.|Ku S.Y.|Lam K.Y.|Huang Y.|Wong W.L.","Chinese University of Hong Kong","Hong Kong","2019","Who is a better teacher for children with autism? Comparison of learning outcomes between robot-based and human-based interventions in gestural production and recognition","Research in Developmental Disabilities","86",NA,"62-75","Children with ASD|Human-based intervention|Intransitive gestures|Robot-based intervention","© 2019 Elsevier Ltd Background: Individuals with autism spectrum disorder (ASD) tend to show deficits in engaging with humans. Previous findings have shown that robot-based training improves the gestural recognition and production of children with ASD. It is not known whether social robots perform better than human therapists in teaching children with ASD. Aims: The present study aims to compare the learning outcomes in children with ASD and intellectual disabilities from robot-based intervention on gestural use to those from human-based intervention. Methods and procedures: Children aged six to 12 with low-functioning autism were randomly assigned to the robot group (N = 12) and human group (N = 11). In both groups, human experimenters or social robots engaged in daily life conversations and demonstrated to children 14 intransitive gestures in a highly-structured and standardized intervention protocol. Outcomes and results: Children with ASD in the human group were as likely to recognize gestures and produce them accurately as those in the robot group in both training and new conversations. Their learning outcomes maintained for at least two weeks. Conclusions and implications: The social cues found in the human-based intervention might not influence gestural learning. It does not matter who serves as teaching agents when the lessons are highly structured.","Article","0"
"10","2-s2.0-85061275623","10.3390/s19030662","30736302","Ullah S.|Mumtaz Z.|Liu S.|Abubaqr M.|Mahboob A.|Madni H.A.","Khwaja Fareed University of Engineering and Information Technology|Southeast University","Pakistan|China","2019","Single-Equipment with Multiple-Application for an Automated Robot-Car Control System","Sensors (Basel, Switzerland)","19","3","","Android|arduino|bluetooth|hand-gesture recognition|low cost|open source|sensors|smart cars|speech recognition","The integration of greater functionalities into vehicles increases the complexity of car-controlling. Many research efforts are dedicated to designing car-controlling systems that allow users to instruct the car just to show it what it should do; however, for non-expert users, controlling the car with a remote or a switch is complicated. So, keeping this in mind, this paper presents an Arduino based car-controlling system that no longer requires manual control of the cars. Two main contributions are presented in this work. Firstly, we show that the car can be controlled with hand-gestures, according to the movement and position of the hand. The hand-gesture system works with an Arduino Nano, accelerometer, and radio-frequency (RF) transmitter. The accelerometer (attached with the hand-glove) senses the acceleration forces that are produced by the hand movement, and it will transfer the data to the Arduino Nano that is placed on hand glove. After receiving the data, Arduino Nano will convert it into different angle values in ranges of 0°<U+207B>450° and send the data to the RF receiver of the Arduino Uno, which is placed on the car through the RF transmitter. Secondly, the proposed car system is to be controlled by an android based mobile-application with different modes (e.g., touch buttons mode, voice recognition mode). The mobile-application system is the extension of the hand-gesture system with the addition of Bluetooth module. In this case, whenever the user presses any of the touch buttons in the application, and/or gives voice commands, the corresponding signal is sent to the Arduino Uno. After receiving the signal, Arduino will check this against its predefined instructions for moving forward, backward, left, right, and brake; then it will send the command to the motor module to move the car in the corresponding direction. In addition, an automatic obstacle detection system is introduced to improve the safety measurements to avoid any hazards with the help of sensors placed at the front of the car. The proposed systems are designed as a lab-scale prototype to experimentally validate the efficiency, accuracy, and affordability of the systems. The experimental results prove that the proposed work has all in one capability (hand-gesture, touch buttons and voice-recognition with mobile-application, obstacle detection), is very easy to use, and can be easily assembled in a simple hardware circuit. We remark that the proposed systems can be implemented under real conditions at large-scale in the future, which will be useful in automobiles and robotics applications.","Article","0"
"11","2-s2.0-85061569402","10.1063/1.5088286",NA,"Xiang D.|Chen Q.|Li Y.","Southwest Petroleum University China","China","2019","Strain sensing behavior of conductive polymer/carbon nanotube composites coated fiber","AIP Conference Proceedings","2065",NA,"","carbon nanotube|coating|fiber|human motion monitoring|polymer composite|strain sensing","© 2019 Author(s). The last decade has witnessed a tremendous growth of research and development in flexible and wearable strain sensors. However, there are still some challenges associated with the fabrication of strain sensors to achieve a high sensitivity and large workable range at low cost. Here, we report on the development of a highly elastic strain sensor based on a commercial spandex fiber coated with a nanocomposite consisting of multi-walled carbon nanotubes (MWCNTs) and thermoplastic polyurethane (TPU) manufactured by a layer-by-layer (LBL) method. The sensor demonstrated outstanding performance with large workable strain, high sensitivity, excellent repeatability and regular signal responses within a wide measuring frequency range of 0.01~1 Hz. Additionally, the effect of ultraviolet irradiation on the sensor performance was also investigated. Applications of the sensor in monitoring diverse human motions, such as facial microexpressions and speech recognition are also demonstrated, showing its potential for applications in wearable devices and intelligent robots.","Conference Paper","0"
"12","2-s2.0-85043480500","10.1016/j.patrec.2018.03.006",NA,"Tapus A.|Bandera A.|Vazquez-Martin R.|Calderita L.V.","Informatique et Ingénierie des Systèmes|Universidad de Malaga","France|Spain","2019","Perceiving the person and their interactions with the others for social robotics – A review","Pattern Recognition Letters","118",NA,"3-13","Human perception|Human–robot interaction|Proxemics|Social interactions|Social robots","© 2018 Elsevier B.V. Social robots need to understand human activities, dynamics, and the intentions behind their behaviors. Most of the time, this implies the modeling of the whole scene. The recognition of the activities and intentions of a person are inferred from the perception of the individual, but also from their interactions with the rest of the environment (i.e., objects and/or people). Centering on the social nature of the person, robots need to understand human social cues, which include verbal but also nonverbal behavioral signals such as actions, gestures, body postures, facial emotions, and proxemics. The correct understanding of these signals helps these robots to anticipate the needs and expectations of people. It also avoids abrupt changes on the human–robot interaction, as the temporal dynamics of interactions are anchored and driven by a major repertoire of social landmarks. Within the general framework of interaction of robots with their human counterparts, this paper reviews recent approaches for recognizing human activities, but also for perceiving social signals emanated from a person or a group of people during an interaction. The perception of visual and/or audio signals allow them to correctly localize themselves with respect to humans from the environment while also navigating and/or interacting with a person or a group of people.","Article","1"
"13","2-s2.0-85054666817","10.1109/TMECH.2018.2875521",NA,"Gupta U.|Wang Y.|Ren H.|Zhu J.","National University of Singapore","Singapore","2019","Dynamic modeling and feedforward control of jaw movements driven by viscoelastic artificial muscles","IEEE/ASME Transactions on Mechatronics","24","1","25-35","Artificial muscles|control|creep|dielectric elastomer actuators (DEAs)|hysteresis|soft robots|viscoelasticity","© 1996-2012 IEEE. Artificial muscles based on dielectric elastomer actuators (DEAs) have been used to mimic the motion of the human jaw. However, DEAs show strong nonlinear behavior coupled with rate dependent viscoelastic phenomena. Under cyclic actuation, the viscoelastic creep coupled with hysteresis further makes control of these viscoelastic membranes difficult. In this paper, we develop a nonlinear dynamic model based on the principles of nonequilibrium thermodynamics to account for viscoelasticity. Furthermore, the damping and inertial effects of the jaw and membrane are taken into account, relating the deformation of the membrane to the voltage applied. Experimental results are found to be consistent with theoretical predictions. The feedforward controller is then developed based on a viscoelastic nonlinear dynamic model. The controller can be used to track the sinusoidal, triangular, and staircase trajectories accurately. Finally, a video demonstrating the jaw movement in the speech, 'I Have a Dream,' by Dr. Martin Luther King Jr. has been shown to illustrate the effectiveness of the controller. This is one of the first efforts to control of soft robots driven by viscoelastic DEAs.","Article","0"
"14","2-s2.0-85058995451","10.1111/pcn.12799","30565801","Grabowski K.|Rynkiewicz A.|Lassalle A.|Baron-Cohen S.|Schuller B.|Cummins N.|Baird A.|Podgórska-Bednarz J.|Pieniazek A.|Lucka I.","Gdanski Uniwersytet Medyczny|University of Rzeszów|Center for Diagnosis|University of Amsterdam|Autism Research Centre|Imperial College London|Universität Augsburg|Association for Children with Attention Deficit Hyperactivity Disorder in Rzeszow|SOLIS RADIUS Association for People with Disabilities and Autism Spectrum Disorders in Rzeszow|Medical Center for Children with Autism Spectrum Disorders in Rzeszow","Poland|Netherlands|United Kingdom|Germany","2019","Emotional expression in psychiatric conditions: New technology for clinicians","Psychiatry and Clinical Neurosciences","73","2","50-62","affective computing|autism|emotions|expressed emotion|nonverbal communication","© 2018 The Authors. Psychiatry and Clinical Neurosciences © 2018 Japanese Society of Psychiatry and Neurology Aim: Emotional expressions are one of the most widely studied topics in neuroscience, from both clinical and non-clinical perspectives. Atypical emotional expressions are seen in various psychiatric conditions, including schizophrenia, depression, and autism spectrum conditions. Understanding the basics of emotional expressions and recognition can be crucial for diagnostic and therapeutic procedures. Emotions can be expressed in the face, gesture, posture, voice, and behavior and affect physiological parameters, such as the heart rate or body temperature. With modern technology, clinicians can use a variety of tools ranging from sophisticated laboratory equipment to smartphones and web cameras. The aim of this paper is to review the currently used tools using modern technology and discuss their usefulness as well as possible future directions in emotional expression research and treatment strategies. Methods: The authors conducted a literature review in the PubMed, EBSCO, and SCOPUS databases, using the following key words: ‘emotions,’ ‘emotional expression,’ ‘affective computing,’ and ‘autism.’ The most relevant and up-to-date publications were identified and discussed. Search results were supplemented by the authors’ own research in the field of emotional expression. Results: We present a critical review of the currently available technical diagnostic and therapeutic methods. The most important studies are summarized in a table. Conclusion: Most of the currently available methods have not been adequately validated in clinical settings. They may be a great help in everyday practice; however, they need further testing. Future directions in this field include more virtual-reality-based and interactive interventions, as well as development and improvement of humanoid robots.","Review","0"
"15","2-s2.0-85055323748","10.1007/s11370-018-0266-9",NA,"Cao H.L.|Jensen L.C.|Nghiem X.N.|Vu H.|De Beir A.|Esteban P.G.|Van de Perre G.|Lefeber D.|Vanderborght B.","Vrije Universiteit Brussel|University of Southern Denmark, Sønderborg","Belgium|Denmark","2019","DualKeepon: a human–robot interaction testbed to study linguistic features of speech","Intelligent Service Robotics","12","1","45-54","Human–robot interaction|Keepon|Linguistics|Low-cost robotics|NAO|Social robot","© 2018, Springer-Verlag GmbH Germany, part of Springer Nature. In this paper, we present a novel dual-robot testbed called DualKeepon for carrying out pairwise comparisons of linguistic features of speech in human–robot interactions. Our solution, using a modified version of the MyKeepon robotic toy developed by Beatbots, is a portable open-source system for researchers to set up experiments quickly, and in an intuitive way. We provide an online tutorial with all required materials to replicate the system. We present two human–robot interaction studies to demonstrate the testbed. The first study investigates the perception of robots using filled pauses. The second study investigates how social roles, realized by different prosodic and lexical speaking profiles, affect trust. Results show that the proposed testbed is a helpful tool for linguistic studies. In addition to the basic setup, advanced users of the system have the ability to connect the system to different robot platforms, i.e., NAO, Pepper.","Article","0"
"16","2-s2.0-85058234094","10.1080/01691864.2018.1555489",NA,"Kühnlenz B.|Kühnlenz K.","Hochschule Coburg","Germany","2019","A dialog switching strategy for information retrieval in noisy environments based on a confidence score","Advanced Robotics","33","1","49-59","dialog strategies|Human–robot interaction|verbal communication","© 2018, © 2018 Informa UK Limited, trading as Taylor  &  Francis Group and The Robotics Society of Japan. This paper presents a dialog switching strategy for information retrieval in human–robot dialog in order to cope with different intensities of background noise. The strategy dynamically switches between a more open and a closed dialog scheme based on a continuously adapted confidence score evaluating resulting numbers of cases of recognition and non-recognition of user speech, which are dependent on the level of background noise. Thereby, more natural and more robust dialog components are balanced, respectively. Experimental results are presented, illustrating the effectiveness of the approach.","Article","0"
"17","2-s2.0-85047994160","10.1109/TNNLS.2018.2830119","29993561","Davila-Chacon J.|Liu J.|Wermter S.","Universität Hamburg|Imperial College London","Germany|United Kingdom","2019","Enhanced Robot Speech Recognition Using Biomimetic Binaural Sound Source Localization","IEEE Transactions on Neural Networks and Learning Systems","30","1","138-150","Automatic speech recognition|behavioral robotics|binaural sound source localization (SSL)|bioinspired neural architectures","© 2012 IEEE. Inspired by the behavior of humans talking in noisy environments, we propose an embodied embedded cognition approach to improve automatic speech recognition (ASR) systems for robots in challenging environments, such as with ego noise, using binaural sound source localization (SSL). The approach is verified by measuring the impact of SSL with a humanoid robot head on the performance of an ASR system. More specifically, a robot orients itself toward the angle where the signal-to-noise ratio (SNR) of speech is maximized for one microphone before doing an ASR task. First, a spiking neural network inspired by the midbrain auditory system based on our previous work is applied to calculate the sound signal angle. Then, a feedforward neural network is used to handle high levels of ego noise and reverberation in the signal. Finally, the sound signal is fed into an ASR system. For ASR, we use a system developed by our group and compare its performance with and without the support from SSL. We test our SSL and ASR systems on two humanoid platforms with different structural and material properties. With our approach we halve the sentence error rate with respect to the common downmixing of both channels. Surprisingly, the ASR performance is more than two times better when the angle between the humanoid head and the sound source allows sound waves to be reflected most intensely from the pinna to the ear microphone, rather than when sound waves arrive perpendicularly to the membrane.","Article","0"
"18","2-s2.0-85048214825","10.1007/978-3-319-78452-6_37",NA,"Suresh A.|Gaba D.|Bhambri S.|Laha D.","New York University|Bharati Vidyapeeth's College of Engineering, New Delhi","United States|India","2019","Intelligent multi-fingered dexterous hand using virtual reality (VR) and robot operating system (ROS)","Advances in Intelligent Systems and Computing","751",NA,"459-474","360 vision|Catkin|Gesture robot|Raspberry pi|ROS|Virtual reality|Voice recognition","© Springer International Publishing AG, part of Springer Nature 2019. The concept of using robots for reducing human effort is now a buzzword since the last decade. Now, with the advancement of the current technologies the robots could be remotely controlled with much greater ease and precision than before in domains, where we could have never even thought of their applications. The paper describes a new user friendly interface which allows the control of the robot manipulator (hand) in the most effective and simplest manner possible. By making use of 360° camera the vision is created which is displayed on the user’s mobile phone (which is placed inside a VR-goggles) and viewed as a virtual reality environment by the user. The user and the robot manipulator are situated far apart and the user will feel like as if the robotic arms are his own arms. Moreover, the user can control the robotic arm placed remotely in any high risk area with or without any internet connectivity. This methodology is very useful for controlling the robotic arm for dealing with radioactive and hazardous materials. Therefore, the user doesn’t have to be in the place of risk and he/she is still able to operate in the most convenient manner. In this paper, a detailed description of the control of the open-source 3D printed robotic hand/forearm from InMoov is discussed. Basically, the InMoov hand/forearm is modified into a useable all-purpose dexterous hand with the application of Virtual Reality to enhance user ease. There are two modes in which the system could be used. First is the hand mimicking mode, in which the user’s hand movements are being replicated onto the robotic hand/forearm, and the second mode is the voice recognition mode in which simple voice commands from the user are processed and are used to control the robotic arm. Voice recognition enables hands free usage of the robotic hand for people with disability. The system makes use of ROS (Robot Operating System) as its baseline program management system. The development is done by making use of ROS catkin workspace on Raspberry Pi 3 with Ubuntu Mate with the implementation of the necessary packages and nodes for every functionality.","Conference Paper","0"
"19","2-s2.0-85048218761","10.1007/978-3-319-78452-6_29",NA,"Ouali C.|Nasr M.M.|AbdelGalil M.A.M.|Karray F.","University of Waterloo|Zewail City of Science and Technology","Canada|Egypt","2019","Voice controlled multi-robot system for collaborative task achievement","Advances in Intelligent Systems and Computing","751",NA,"345-360",NA,"© Springer International Publishing AG, part of Springer Nature 2019. This paper describes a framework for machine cooperation and interaction designed to promote collaborative work with humans in a natural and flexible manner. More specifically, we propose a system where all the interactions with the robots are achieved through voice using Alexa Voice Service (AVS). Besides the implementation of a voice interface channel, we develop a navigation module that enables a robot to autonomously navigate through a map to reach a specific destination while avoiding obstacles. We provide the system with the ability to choose and operate the appropriate robot in context of multi-robot cooperation for task achievement. In addition, a user can intervene during the process of executing a relatively complex task by interacting with the robots via voice commands. To validate the proposed framework, we conduct experiments to quantify the performance during cooperative action involving the human and the system.","Conference Paper","0"
"20","2-s2.0-85048219057","10.1007/978-3-319-78452-6_20",NA,"Al-Khulaidi R.A.|Bakr N.H.B.A.|Fauzi N.M.|Akmeliawati R.","International Islamic University Malaysia","Malaysia","2019","Signbot, sign-language performing robot, based on sequential motion of servo motor arrays","Advances in Intelligent Systems and Computing","751",NA,"239-249",NA,"© Springer International Publishing AG, part of Springer Nature 2019. This paper presents the development of SignBot, a 3D printed robot which can perform Malaysian Sign Language (MSL). This work is the first attempt to eliminate the barriers of communication between hearing impaired individuals and the mainstream society as it is the first robot to perform MSL. The signing, in this work, can be done with two hands. The robot hands were developed with detailed finger joints. Micro servo motors were installed to allow for the signing motions for the relevant joints of selected letters, numbers as well as phrases for emergency cases. The sequential movements of the servo motor arrays are stored in the database to represent particular signs, and retrieved by the microcontroller based on the speech input detected, and finally executed accordingly by the servo motors to perform the sign.","Conference Paper","0"
"21","2-s2.0-85048221715","10.1007/978-3-319-78452-6_11",NA,"Gudi S.L.K.C.|Ojha S.|Sidra |Johnston B.|Williams M.A.","University of Technology Sydney","Australia","2019","A proactive robot tutor based on emotional intelligence","Advances in Intelligent Systems and Computing","751",NA,"113-120","AUSLAN|ECS|Emotions|Sign language|Social robots","© Springer International Publishing AG, part of Springer Nature 2019. In recent years, social robots are playing a vital role in various aspects of acting as a companion, assisting in regular tasks, health, interaction, teaching, etc. Coming to the case of robot tutor, the actions of the robot are limited. It may not fully understand the emotions of the student. It may continue to give lecture even though the user is bored or left away from the robot. This situation makes a user feel that robot cannot supersede a human being because it is not in a position to understand emotions. To overcome this issue, in this paper, we present an Emotional Classification System (ECS) where the robot adapts to the mood of the user and behaves accordingly by becoming proactive. It works based on the emotion tracked by the robot using its emotional intelligence. A robot as a sign language tutor scenario is considered to assist speech and hearing impairment people for validating our model. Real-time implementations and analysis are further discussed by considering Pepper robot as a platform.","Conference Paper","0"
"22","2-s2.0-85048239284","10.1007/978-3-319-78452-6_24",NA,"Thinh N.T.|Tho T.P.|Toai T.C.|Ben L.T.","HCMC University of Technology and Education","Viet Nam","2019","Robot supporting for deaf and less hearing people","Advances in Intelligent Systems and Computing","751",NA,"283-289",NA,"© Springer International Publishing AG, part of Springer Nature 2019. This paper discusses the development of a service robot for translating spoken language text into signed languages and vice versa The motivation for our study is the improvement of accessibility to public information announcements for deaf and less hearing people. The robot can translate Vietnamese sign language into speech and recognize Vietnamese/English speech to suitable gesture/sign language. The paper describes the use of service robot in a sign language machine translation system. Several sign language visualization methods were evaluated on the robot. In order to perform this study a machine translation service robot that uses display screen on robot as service-delivery device was developed as well as a 3D avatar. It was concluded that service robot are suitable service-delivery platforms for sign language machine translation systems.","Conference Paper","0"
"23","2-s2.0-85048246527",NA,NA,"NA NA","NA","NA","2019","5th International Conference on Robot Intelligence Technology and Applications, RiTA 2017","Advances in Intelligent Systems and Computing","751",NA,"",NA,"The proceedings contain 46 papers. The special focus in this conference is on Robot Intelligence Technology and Applications. The topics include: A proactive robot tutor based on emotional intelligence; reusability quality metrics for agent-based robot systems; SVM-based fault type classification method for navigation of formation control systems; Indoor magnetic pose graph SLAM with Robust Back-End; Multi-robot SLAM: An overview and quantitative evaluation of MRGS ROS framework for MR-SLAM; a reinforcement learning technique for autonomous back driving control of a vehicle; rapid coverage of regions of interest for environmental monitoring; embedded drilling system using rotary-percussion drilling; design and development of innovative pet feeding robot; reinforcement learning of a memory task using an Echo State network with multi-layer readout; signbot, sign-language performing robot, based on sequential motion of servo motor arrays; isolated bio-regenerative system for vertical farming through robots in space explorations; a mobile social bar table based on a retired security robot; i remember what you did: A behavioural guide-robot; robot supporting for deaf and less hearing people; the use of machine learning for correlation analysis of sentiment and weather data; design and analysis of a novel sucked-type underactuated hand with multiple grasping modes; robotizing the bio-inspiration; an observer design technique for improving velocity estimation of a gimbal system; voice controlled multi-robot system for collaborative task achievement; selective filter transfer; the control of an upper extremity exoskeleton for stroke rehabilitation by means of a hybrid active force control; intellectual multi agent control of tripod; multi-object tracking with pre-classified detection.","Conference Review","0"
"24","2-s2.0-85049010183","10.1007/978-3-319-92231-7_16",NA,"Sun C.|Zhang J.|Liu C.|King B.C.B.|Zhang Y.|Galle M.|Spichkova M.|Simic M.","RMIT University","Australia","2019","Software development for autonomous and social robotics systems","Smart Innovation, Systems and Technologies","98",NA,"151-160",NA,"© Springer International Publishing AG, part of Springer Nature 2019. One of the core features of social robotics system is a physical interaction between humans and humanoid robots. This provides additional challenges, both from safety and usability prospectives. When dealing with human-robot interaction, human safety has the highest priority. While in industrial environment we have robot cells to protect humans, in social robotics, that we consider, physical contact is possible, as well as other interactions, with consequences that might be in psychological areas. For example, the conversation with children might have different requirements in comparison to the conversation with adults, the behavioural assumptions might be different, etc. This paper summarises the core results of a project on social robotics system, where an autonomous humanoid robot guides visitors through a lab tour. The results of our work were implemented on the humanoid PAL REEM robot. The implementation includes a web-application to support the management of robot-guided tours. The application also provides recommendations for the users as well as allows for a visual analysis of historical data on the tours.","Conference Paper","0"
"25","2-s2.0-85049509088","10.1007/978-3-319-94346-6_14",NA,"Hernandez-Cedeño J.|Ramírez-Benavides K.|Guerrero L.|Vega A.","Universidad de Costa Rica","Costa Rica","2019","NAO as a copresenter in a robotics workshop - Participant’s feedback on the engagement level achieved with a robot in the classroom","Advances in Intelligent Systems and Computing","784",NA,"143-152","Education robots|Human-robot collaboration|Human-robot interaction|Humanoid robots","© Springer International Publishing AG, part of Springer Nature 2019. Robotics, combined with computer science and human-centered studies, can have a substantial impact in areas such as education and innovation. Robots have proven to be a good tool to gain and maintain users’ involvement in different activities. In education, robots can be used as teaching assistants to improve participation, enhance concentration or just to get students’ attention. In this research, we involved an NAO, a humanoid robot, in a workshop presentation with the aim of measuring the impact of this technique on the level of engagement showed by the participants. The robot was programmed to simulate speech and gesticulate while it talked to apply the Wizard of Oz technique.","Conference Paper","0"
"26","2-s2.0-85049523363","10.1007/978-3-319-94346-6_7",NA,"Dao Q.V.|Martin L.|Mercer J.|Wolter C.|Gomez A.|Homola J.","NASA Ames Research Center|San Jose State University","United States","2019","Information displays and crew configurations for UTM operations","Advances in Intelligent Systems and Computing","784",NA,"64-74","Situation awareness|Teams|UAS|UTM","© Springer International Publishing AG, part of Springer Nature 2019. In this paper we discuss how team configuration may influence how information is shared among team members for low-altitude Unmanned Aircraft Systems (UAS) operations. NASA collected and analyzed observation data gathered during a series of field tests for the UAS Traffic Management (UTM) project. The field tests were part of a larger effort aimed at advancing the UTM concept, conducted at six test-sites across the USA. Ground control station (GCS) concepts, flight-crew composition, and crew-size varied within and across test-sites. Flight crews took two strategic approaches to organizing their teams. The first of the two approaches was implemented by one third of the flight crews. These crews integrated the role of UTM operator into the duties of existing crew members, merging the current roles with this new one, keeping the UTM operator collocated with the flight crew. The remaining two thirds implemented a distributed team configuration, where a single UTM operator distributed support across multiple crews. Results from our data collection efforts revealed that UTM operator location influenced whether flight crews used verbal communication versus displays to acquire UTM information.","Conference Paper","0"
"27","2-s2.0-85049537671","10.1007/978-3-319-94622-1_12",NA,"Serpa-Andrade G.|Serpa-Andrade L.|Robles-Bykbaev V.","Universidad Politécnica Salesiana - UPS","Ecuador","2019","Preprocessing the structural optimization of the SPELTRA robotic assistant by numerical simulation based on finite elements","Advances in Intelligent Systems and Computing","776",NA,"116-127","Children with disabilities|Communication disorders|Finite elements|Pedagogical robotic|Simulation by numerical methods|Special education|SPELTRA|Structural design","© Springer International Publishing AG, part of Springer Nature 2019. This project will structurally optimize the robotic assistant SPELTRA (Robotic Assistant for Speech and Language Therapy) that serves as pedagogical support to children with and without disabilities who are benefiting through the UNESCO Chair and Assistance Technologies (GI-IATa) of the UPS. This process would be carrying out through the study of different geometries and materials considering certain aspects technical and economic that will help to define by weighting parameters the optimal geometry and two possible materials for the design and creation of the new structure of the robot. After, will be defined all the variables and restrictions affecting the SPELTRA during the manipulation to which it is subjected in the robot-human interaction, thus establishing the structural model. For its analysis, CAD-CAE computational tools based on finite elements will be using and it will can to observe the possible deformations, efforts and safety factor to which would be subjected.","Conference Paper","0"
"28","2-s2.0-85051704338","10.1007/978-3-319-96080-7_42",NA,"Ipsen C.|Nardelli G.|Poulsen S.|Ronzoni M.","Danmarks Tekniske Universitet","Denmark","2019","Implementing Tele Presence Robots in Distance Work: Experiences and Effects on Work","Advances in Intelligent Systems and Computing","821",NA,"358-365","Distance management|Implementation|Telepresence robots","© 2019, Springer Nature Switzerland AG. As companies move toward globalization, companies use distance work to accomplish work more effectively and efficiently. A telepresence robot (TPR) is a mobile remote presence device that allows a two-way communication and interaction between a distance manager and the employees. The objective of the study was to improve the understanding of how distance workers and managers experience the use of TPR in the daily management and in which tasks the TPR is suitable to ensure employee well-being and thus performance. The data collection included three phases – before, during and after the implementation of the TPR, where we conducted 25 semi-structured individual and group interviews, on-site observations of the TPR in use and research notes. The distance manager (user) controlled the TPR from a distant site when using it in the home office. The managers were able to create a sense of proximity and via the camera feature, enable eye-contact, which the managers considered essential and beneficial for assessing the employee’s feelings and well-being. The majority of the users had a positive experience regarding the TPR basic functionalities´ utilization. In all three cases the participants, both managers and employees, agreed that the TPR is most useful in planned project meetings. On the other hand, the lack of trust, problems with the technology, privacy issues and intrusive emotions affected the use of the TPR in a negative way in some cases. The TPR was not suitable for meetings where people needed to share physical documents or important meetings, i.e. private talks or decisions meetings.","Conference Paper","0"
"29","2-s2.0-85051783599","10.1007/978-3-319-92108-2_13",NA,"Kampman O.|Siddique F.B.|Yang Y.|Fung P.","Hong Kong University of Science and Technology|EMOS Technologies, Inc.","Hong Kong","2019","Adapting a virtual agent to user personality","Lecture Notes in Electrical Engineering","510",NA,"111-118","Adaptive virtual agents|Empathetic robots|Personality recognition","© 2019, Springer International Publishing AG, part of Springer Nature. We propose to adapt a virtual agent called ‘Zara the Supergirl’ to user personality. User personality is deducted through two models, one based on raw audio and the other based on speech transcription text. Both models show good performance, with an average F-score of 69.6 for personality perception from audio, and an average F-score of 71.0 for recognition from text. Both models deploy a Convolutional Neural Network. Through a Human-Agent Interaction study we find correlations between user personality and preferred agent personality. The study suggests that especially the Openness user personality trait correlates with a stronger preference for agents with more gentle personality. People also sense more empathy and enjoy better conversations when agents adapt their personalities.","Conference Paper","0"
"30","2-s2.0-85051788197",NA,NA,"NA NA","NA","NA","2019","8th International Workshop on Spoken Dialogue Systems, IWSDS 2017","Lecture Notes in Electrical Engineering","510",NA,"",NA,"The proceedings contain 25 papers. The special focus in this conference is on Spoken Dialogue Systems. The topics include: Towards metrics of evaluation of pepper robot as a social companion for the elderly; a social companion and conversational partner for the elderly; adapting a virtual agent to user personality; A conversational dialogue manager for the humanoid Robot ERICA; eliciting positive emotional impact in dialogue response selection; predicting interaction quality in customer service dialogs; direct and mediated interaction with a holocaust survivor; acoustic-prosodic entrainment in multi-party spoken dialogues: Does simple averaging extend existing pair measures properly?; how to find the right voice commands? Semantic priming in task representations; automatic evaluation of chat-oriented dialogue systems using large-scale multi-references; exploring the applicability of elaborateness and indirectness in dialogue management; an open-source dialog system with real-time engagement tracking for job interview training applications; on the applicability of a user satisfaction-based reward for dialogue policy learning; OntoVPA—An ontology-based dialogue management system for virtual personal assistants; regularized neural user model for goal-oriented spoken dialogue systems; yeah, right, uh-huh: A deep learning backchannel predictor; what information should a dialogue system understand?: Collection and analysis of perceived information in chat-oriented dialogue; chunks in multiparty conversation—building blocks for extended social talk; debbie, the debate bot of the future; data-driven dialogue systems for social agents; chaD: Chat-oriented dialog systems; domain complexity and policy learning in task-oriented dialogue systems; single-model multi-domain dialogue management with deep learning.","Conference Review","0"
"31","2-s2.0-85051790509","10.1007/978-3-319-96074-6_16",NA,"Reinhardt J.|Schmidtler J.|Bengler K.","Technical University of Munich","Germany","2019","Corporate robot motion identity","Advances in Intelligent Systems and Computing","823",NA,"152-164","Corporate identity|Human-robot interaction|Legibility|Motion planning|Movement cues|Service design|Service robots","© Springer Nature Switzerland AG 2019. Mobile robotic systems are increasingly merging into human dominated areas and therefore will interact and coordinate with pedestrians in private and public spaces. To ease intuitive coordination in human-robot interaction, robots should be able to express intent via motion. This will enable an observer to quickly and confidently infer the robot’s goal to establish productive encounters. For long-term interaction, trajectories in straight drive or curvature have been optimized for this purpose. In addition, short-term movement cues are perceivable changes in motion parameters and direction of movement that can be utilized to express intent in a non-verbal manner. For example, yielding priority to a person via a short back-off movement cue, as opposed to merely a stop, provides the possibility of legible and agreeable robot navigation. In the service design domain, the front line personnel’s behavior is a crucial quality factor of how an organization is perceived by customers and society. Recent developments show that mobile robotic systems are increasingly supplementing a service company’s front line personnel. Companies such as Starship Technologies or Deutsche Post apply service robots for transportation purposes. Integrating robot motion into an organization’s visual identity to communicate the visual cues of what the organization wants to express could contribute to the customer experience. In order to provide movement cues that are not only legible, but convey an inherent personality of the robot carrying out the task and therefore reflect on the organization’s public image, we discuss aforementioned factors for consideration when developing a corporate robot motion identity. We integrate service quality domains and affected human roles for application in the creative practice of designing motion. Thus, recognizable movement cues which are designed to express intent to coexisting and cooperating pedestrians in an everyday context can be tailored to what an organization wants to express to its environment, customers or other stakeholders.","Conference Paper","0"
"32","2-s2.0-85051814557","10.1007/978-3-319-96059-3_4",NA,"Rothwell C.D.|Shalin V.L.|Romigh G.D.","Wright State University|Infoscitex Corporation|Kno.e.sis Center|Wright-Patterson AFB","United States","2019","Searching for the model of common ground in human-computer dialogue","Advances in Intelligent Systems and Computing","827",NA,"33-42","Common ground|Dialogue|Human-computer interaction","© Springer Nature Switzerland AG 2019. Natural language dialogue is a desirable method for human-robot interaction and human-computer interaction. Critical to the success of dialogue is the underlying model for common ground and the grounding process that establishes, adds to, and repairs shared understanding. The model of grounding for human-computer interaction should be informed by human-human dialogue. However, the processes involved in human-human grounding are under dispute within the research community. Three models have been proposed: alignment, a simple model that has been influential on dialogue system development, interpersonal synergy, an automatic coordination emerging from interaction, and perspective taking, a strategic interaction based on intentional coordination. Few studies have simultaneously evaluated these models. We tested the models’ ability to account for human-human performance in a complex collaborative task that stressed the grounding process. The results supported the perspective taking model over the synergy model and the alignment model, indicating the need to reassess the alignment model as a foundation for human-computer interaction.","Conference Paper","0"
"33","2-s2.0-85051821641","10.1007/978-3-319-92108-2_11",NA,"Bechade L.|Dubuisson-Duplessis G.|Pittaro G.|Garcia M.|Devillers L.","Universite Paris-Sud XI|Sorbonne Universite","France","2019","Towards metrics of evaluation of pepper robot as a social companion for the elderly","Lecture Notes in Electrical Engineering","510",NA,"89-101","Data collection|Elderly end-users|Evaluation|Human-Robot Interaction|Metrics","© 2019, Springer International Publishing AG, part of Springer Nature. For the design of socially acceptable robots, field studies in Human-Robot Interaction are necessary. Constructing dialogue benchmarks can have a meaning only if researchers take into account the evaluation of robot, human, and their interaction. This paper describes a study aiming at finding an objective evaluation procedure of the dialogue with a social robot. The goal is to build an empathic robot (JOKER project) and it focuses on elderly people, the end-users expected by ROMEO2 project. The authors carried out three experimental sessions. The first time, the robot was NAO, and it was with a Wizard of Oz (emotions were entered manually by experimenters as inputs to the program). The other times, the robot was Pepper, and it was totally autonomous (automatic detection of emotions and decision according to). Each interaction involved various scenarios dealing with emotion recognition, humor, negotiation and cultural quiz. The paper details the system functioning, the scenarios and the evaluation of the experiments.","Conference Paper","0"
"34","2-s2.0-85051825763","10.1007/978-3-319-92108-2_14",NA,"Milhorat P.|Lala D.|Inoue K.|Zhao T.|Ishida M.|Takanashi K.|Nakamura S.|Kawahara T.","Kyoto University","Japan","2019","A conversational dialogue manager for the humanoid Robot ERICA","Lecture Notes in Electrical Engineering","510",NA,"119-131","Attentive listening|Human robot interaction|Statement response|User studies","© 2019, Springer International Publishing AG, part of Springer Nature. We present a dialogue system for a conversational robot, Erica. Our goal is for Erica to engage in more human-like conversation, rather than being a simple question-answering robot. Our dialogue manager integrates question-answering with a statement response component which generates dialogue by asking about focused words detected in the user’s utterance, and a proactive initiator which generates dialogue based on events detected by Erica. We evaluate the statement response component and find that it produces coherent responses to a majority of user utterances taken from a human-machine dialogue corpus. An initial study with real users also shows that it reduces the number of fallback utterances by half. Our system is beneficial for producing mixed-initiative conversation.","Conference Paper","0"
"35","2-s2.0-85052249168",NA,NA,"NA NA","NA","NA","2019","International Conference on Cognitive Informatics and Soft Computing, CISC 2017","Advances in Intelligent Systems and Computing","768",NA,"",NA,"The proceedings contain 74 papers. The special focus in this conference is on Cognitive Informatics and Soft Computing. The topics include: Data mining approaches for correlation and cache replacement in wireless ad hoc networks; an approach to detect an image as a selfie using object recognition methods; t wave analysis: Potential marker of arrhythmia and ischemia detection-a review; functional link artificial neural network (flann) based design of a conditional branch predictor; development of a model recommender system for agriculture using apriori algorithm; emotion speech recognition based on adaptive fractional deep belief network and reinforcement learning; concept-based electronic health record retrieval system in healthcare iot; hybrid cat using bayes classification and two-parameter model; a change detection technique using rough c-means on medical images; categorizing kidney stones using region properties and pixel intensity matrix; malware architectural view with performance analysis in network at its activation state; non-linear analysis of time series generated from the freeman k-set model; novel approach to segment the pectoral muscle in the mammograms; performance analysis of tcp variants using routing protocols of manet in grid topology; an ensemble feature selection framework of sonar targets using symmetrical uncertainty and multi-layer perceptron (su-mlp); autonomous path guiding robot for visually impaired people; analysis of diabetes for indian ladies using deep neural network; deep neural network based speech enhancement; wind power forecasting using support vector machine model in rstudio; a survey: Classification of big data; security measures in distributed approach of cloud computing; review paper: Licence plate and car model recognition.","Conference Review","0"
"36","2-s2.0-85052300241","10.1007/978-3-319-96071-5_119",NA,"Khalid H.|Liew W.S.|Voong B.S.|Helander M.","Damai Sciences|University of Malaya","Malaysia","2019","Creativity in Measuring Trust in Human-Robot Interaction Using Interactive Dialogs","Advances in Intelligent Systems and Computing","824",NA,"1175-1190","Dialog design|Human-Robot interaction|Trust","© 2019, Springer Nature Switzerland AG. The measurement of human trust in humanoid robots in human-robot interaction requires novel approaches that can predict trust effectively. We present a method that mapped subjective measures (i.e. general trust, psychological) to objective measures (i.e. physiological) to predict trust. We designed interactive dialogs that represent real world service scenarios of Business, Disaster, and Healthcare. The dialogs embedded fifteen trust attributes of Ability, Benevolence and Integrity (ABI) in the communication dialogs. The ABI measures were mapped to physiological measures of facial expressions, voiced speech and camera-based heart rate. Forty-eight subjects comprising 24 males and 24 females aged between 18 to 36 years participated in the experiment. Half of the subjects were Malays and half were Chinese. Three humanoid robots represented full bodied, partial bodied and virtual agents. The experimental design was a within-subjects design. Each subject was tested on all robots in all scenarios. Subjects scored trust on an online scale that ranged from 0 to 7 points. The subjective data was analyzed using Univariate and Oneway MANOVA. The results found the humanoids to be trustworthy in different service tasks. The attributes of ‘Integrity’ and ‘Ability’ trust components are important in Business and Disaster scenarios. The estimation of trust was about 83% accurate when using this creative approach. In conclusion, humanoid robots can interact with humans using dialogs that are representative of real world communication.","Conference Paper","0"
"37","2-s2.0-85052318065","10.1007/978-3-319-96071-5_13",NA,"Lee K.H.|Chua K.W.L.|Koh D.S.M.|Tan A.L.S.","DSO National Laboratories","Singapore","2019","Team Cognitive Walkthrough: Fusing Creativity and Effectiveness for a Novel Operation","Advances in Intelligent Systems and Computing","824",NA,"117-126","Cognitive walkthrough|Collaboration|Design thinking","© 2019, Springer Nature Switzerland AG. For first-of-its kind operation, it is valuable to bring together system designers and combatants to generate creative and feasible concepts to deploy robots in future operations. System designers will share the perspective in the terms of the technological possibilities and system limitations while the combatants bring with them the rich battlefield knowledge. The proposed methodology is the “Team Cognitive Walkthrough” which leverages on the strengths of both Design Thinking and Cognitive Walkthrough techniques to encourage collaboration to derive concepts for a novel operation. The Team Cognitive Walkthrough is conducted over a series of 3 day-long workshops with participants, made up of an equal mix of soldiers and engineers to define the problem statement, identify opportunities, and co-create a suite of solutions. Through weaving Design Thinking methodologies into Cognitive Walkthrough, creativity and effectiveness can both be achieved at an early stage of the development. LEGO ® SERIOUS PLAY ® provides a common language through the use of LEGO® bricks to establish rapport and common understanding amongst the participants. Collaborative Sketch allowed all participants to have equal voices in the first round of design. Application of the low fidelity paper prototypes during the Cognitive Walkthrough meant that the design changes could be effected rapidly. The initial findings have indicated that the ideation workshop involving the combatants and engineers together had led to the derivation of creative solutions. The combatants acquired a better understanding of the technological challenges while the engineers gained a deeper appreciation of the battlefield considerations.","Conference Paper","0"
"38","2-s2.0-85052661815","10.1016/j.future.2018.08.006",NA,"Hu L.|Miao Y.|Wu G.|Hassan M.M.|Humar I.","Huazhong University of Science and Technology|King Saud University|University of Ljubljana","China|Saudi Arabia|Slovenia","2019","iRobot-Factory: An intelligent robot factory based on cognitive manufacturing and edge computing","Future Generation Computer Systems","90",NA,"569-577","Artificial intelligence|Emotion interaction|Quality of experience","© 2018 Elsevier B.V. The Internet of Things (IoT) and Artificial Intelligence (AI) have been driving forces in propelling the technical innovation of intelligent manufacturing, promoting economic growth, and improving the quality of people's lives. In an intelligent factory, introducing edge computing is conducive to expanding the computing resources, the network bandwidth, and the storage capacity of the cloud platform to the IoT edge, as well as realizing the resource scheduling and data uplink and downlink processing during the manufacturing and production processes. Moreover, the emotion recognition and interaction of the Affective Interaction Intelligence Robot (iRobot), with the IoT cloud platform as the infrastructure and AI technology as the core competitiveness, can better solve the psychological problems of the user. Accordingly, this has become a hot research topic in the field of intelligent manufacturing. In this paper, we describe an intelligent robot factory (iRobot-Factory), adopt a highly interconnected and deeply integrated intelligent production line, and introduce the overall structure, composition, characteristics, and advantages of such a factory in details from the two aspects of cognitive manufacturing and edge computing. Then, we describe the implementation of the volume production of iRobot using iRobot-Factory and look at the system performance experimental results and analysis of the iRobot-Factory and a traditional factory. The experimental results show that our scheme significantly improved both the chip assembly and the production efficiency, while the number of system instructions also decreased significantly. In addition, we discuss some open issues relating to cloud-end fusion, load balancing, and personalized robots to make reference to promoting the emotion recognition and interaction experience of users.","Article","3"
"39","2-s2.0-85052795719","10.1007/978-3-030-00365-4_44",NA,"Pencic M.|Cavic M.|Borovac B.|Lu Z.","University of Novi Sad|Changshu Institute of Technology|Shenyang Institute of Automation Chinese Academy of Sciences","Serbia|China","2019","Social humanoid robot SARA: Development and realization of the shrug mechanism","Mechanisms and Machine Science","66",NA,"369-377","Kinematic-dynamic analysis|Mechanical design|Non-verbal communication|Optimal synthesis|Shrug mechanism","© 2019, Springer Nature Switzerland AG. One of the basic requirements of socially interactive robots is that they are able to effectively communicate, verbally and non-verbally. In addition to communication via speech, non-verbal communication of the robot is of utmost importance enabling different informations to be communicated in a simple and intuitive manner as well as expressing emotions. The paper presents the development and realization of the shrug mechanism as an addition to the assortment of non-verbal communication of socially interactive robots. The realized shrug mechanism has 1 DOF and enables lifting/lowering both shoulders together. It consists of a ballscrew shaft/nut mechanism that is coupled with a six-link planar lever mechanism. Within the kinematic-dynamic analysis, the basic parameters of the lever mechanism are defined – driving force on the input link, the vertical stroke of the end point of the shoulder and the dynamic efficiency of the lever mechanism. By optimal synthesis the driving force on the input link is reduced and, at the same time, the stroke of the input link is the smallest, while the stroke of the output links is the largest. That is why the movement of shoulders shrug is fast and short-termed, which was the basic requirement for realization of a movement that largely resembles to human-like movement. In addition, the mechanism is efficient in all positions during motion because the values of the transmission angle are within the prescribed values for the lever mechanisms. Ballscrew shaft/nut mechanism, besides enabling additional torque reduction, is also low backlash which is significant for motion control. The realized shrug mechanism enables high speed of shoulder shrug for small driving torque, has low backlash and high efficiency, compact design and small mass and dimensions.","Chapter","0"
"40","2-s2.0-85052876146","10.1007/978-3-319-97550-4_10",NA,"Garcia J.A.|Lima P.U.","Instituto Superior Técnico","Portugal","2019","Improving human behavior using pomdps with gestures and speech recognition","Intelligent Systems, Control and Automation: Science and Engineering","94",NA,"145-163",NA,"© 2019, Springer Nature Switzerland AG. This work proposes a decision-theoretic approach to problems involving interaction between robot systems and human users, with the goal of estimating the human state from observations of its behavior, and taking actions that encourage desired behaviors. The approach is based on the Partially Observable Markov Decision Process (POMDP) framework, which determines an optimal policy (mapping beliefs onto actions) in the presence of uncertainty on the effects of actions and state observations, extended with information rewards (POMDP-IR) to optimize the information-gathering capabilities of the system. The POMDP observations consist of human gestures and spoken sentences, while the actions are split into robot behaviors (such as speaking to the human) and information-reward actions to gain more information about the human state. Under the proposed framework, the robot system is able to actively gain information and react to its belief on the state of the human (expressed as a probability mass function over the discrete state space), effectively encouraging the human to improve his/her behavior, in a socially acceptable manner. Results of applying the method to a real scenario of interaction between a robot and humans are presented, supporting its practical use.","Chapter","0"
"41","2-s2.0-85053072416","10.1016/j.jss.2018.07.040","30502270","Chen H.E.|Yovanoff M.A.|Pepley D.F.|Prabhu R.S.|Sonntag C.C.|Han D.C.|Moore J.Z.|Miller S.R.","Pennsylvania State University|Penn State Health Milton S. Hershey Medical Center|Penn State College of Medicine","United States","2019","Evaluating surgical resident needle insertion skill gains in central venous catheterization training","Journal of Surgical Research","233",NA,"351-359","Central venous catheterization|Needle insertion|Robotic simulation|Surgical simulation|Training and assessment|Ultrasound manikins","© 2018 Elsevier Inc. Background: Training for ultrasound-guided central venous catheterization (CVC) is typically conducted on static manikin simulators with real-time feedback from a skilled observer. Dynamic haptic robotic trainers (DHRTs) are an alternative method that simulates various patient anatomies and provides consistent feedback for each insertion. This study evaluates CVC needle insertion efficiency and skill gains of both methods. Materials and methods: Fifty-two first-year surgical residents were trained by placing internal jugular (IJ) CVC needles in manikins (n = 26) or robots (n = 26). Manikin-trained participants received verbal feedback from an experienced observer, whereas robotically trained participants received quantitative feedback from the personalized DHRT learning interface. All participants were pretested on a Blue Phantom manikin; participants completed posttesting on a Blue Phantom manikin (n = 26) or a novel manikin (n = 26) with different vessel depth and position. During pretests and posttests residents were timed, motion-tracked, and scored on an IJ CVC checklist. Results: (1) All skills on the IJ CVC checklist showed significant (P < 0.014) improvements from pretests to posttest; (2) Average angle of insertion, path length, and jerk improved significantly (P < 0.005); (3) Average procedural completion time, with standard error (SE) reported, decreased significantly from pretest (M = 3.516 min, SE = 0.277) to posttest (M = 1.997, SE = 0.409). Conclusions: No significant group differences were observed in overall skill gains, but residents’ average procedural completion time decreased significantly from pretests to posttest. Overall results support DHRT as an effective method for training IJ CVC skills.","Article","0"
"42","2-s2.0-85053195906","10.1007/978-3-319-99316-4_27",NA,"Kudryavtsev K.Y.|Cherepanov A.V.|Voznenko T.I.|Dyumin A.A.|Gridnev A.A.|Chepin E.V.","National Research Nuclear University MEPhI|National University of Science &amp; Technology (MISIS)","Russian Federation","2019","The Method of Statistical Estimation of the Minimum Number of Tests for Reliable Evaluation of the Robotic Multi-channel Control Systems Quality","Advances in Intelligent Systems and Computing","848",NA,"203-208","Multi-channel control system|Reliability evaluation|Robotics","© 2019, Springer Nature Switzerland AG. There is a growth in adoption of multi-channel (tactile, voice, gesture, brain-computer interface, etc.) control systems for mobile robots that help to improve its reliability. But, a complex control system requires a large number of tests to determine the quality of operation. Hence, multi-channel control systems are subjects of rigorous testing process for estimation of the number of successful command recognitions and the number of errors. The more tests will be conducted, the more accurate evaluation of the control channel quality will be. However, in most cases, carrying out the tests is expensive and time-consuming. Therefore, it is necessary to determine the minimum number of tests required to evaluate channel control quality with a given significance level. In this paper we propose a technique for determining the minimum required number of tests. Experimental results of evaluating the multichannel control system of the mobile robotic wheelchair using this technique are presented.","Conference Paper","0"
"43","2-s2.0-85053255511",NA,NA,"NA NA","NA","NA","2019","5th International Conference on Computational Science and Technology, ICCST 2018","Lecture Notes in Electrical Engineering","481",NA,"",NA,"The proceedings contain 55 papers. The special focus in this conference is on . The topics include: Smart verification algorithm for IoT applications using QR tag; daily activities classification on human motion primitives detection dataset; implementation of quarter-sweep approach in poisson image blending problem; autonomous road potholes detection on video; Performance comparison of sequential and cooperative integer programming search methodologies in solving curriculum-based university course timetabling problems (CB-UCT); A framework for linear TV recommendation by leveraging implicit feedback; study of adaptive model predictive control for cyber-physical home systems; implementation of constraint programming and simulated annealing for examination timetabling problem; time task scheduling for simple and proximate time model in cyber-physical systems; towards stemming error reduction for Malay texts; DDoS attack monitoring using smart controller placement in software defined networking architecture; Malay language speech recognition for preschool children using hidden markov model (HMM) system Training; a formal model of multi-agent system for university course timetabling problems; an investigation towards hostel space allocation problem with stochastic algorithms; sensor selection based on minimum redundancy maximum relevance for activity recognition in smart homes; improving network service fault prediction performance with multi-instance learning; identification of road surface conditions using IoT sensors and machine learning; Real-time optimal trajectory correction (ROTC) for autonomous omnidirectional robot; incorporating cellular automaton based microscopic pedestrian simulation and genetic algorithm for spatial layout design optimization; qoE enhancements for video traffic in wireless networks through selective packet drops; Contactless palm vein ROI extraction using convex hull algorithm; residential neighbourhood security using WiFi.","Conference Review","0"
"44","2-s2.0-85053266882",NA,NA,"NA NA","NA","NA","2019","International Conference on Computer, Communication and Computational Sciences, IC4S 2017","Advances in Intelligent Systems and Computing","759",NA,"",NA,"The proceedings contain 85 papers. The special focus in this conference is on . The topics include: Traffic surveillance video summarization for detecting traffic rules violators using R-CNN; GAE: A genetic-based approach for software workflow improvement by unhiding hidden transactions of a legacy application; Research on object detection algorithm based on PVANet; Characterization of groundwater contaminant sources by utilizing MARS based surrogate model linked to optimization model; performance comparisons of socially inspired metaheuristic algorithms on unconstrained global optimization; comparative analysis of prediction algorithms for diabetes; An improved discrete grey model based on BP neural network for traffic flow forecasting; influence of the system parameters on the final accuracy for the user identification by free-text keystroke dynamics; correlation criterion in assessment of speech quality in process of oncological patients rehabilitation after surgical treatment of the speech-producing tract; Analysis of online news popularity and bank marketing using ARSkNN; Compressive sensing classifier based on K-SVD; Nonlinear manifold classification based on LLE; application of deep autoencoders in commerce recommendation; An efficient pipelined feedback processor for computing a 1024-Point FFT using distributed logic; Design of low-voltage CMOS Op-Amp using evolutionary optimization techniques; RGA-based wide null control for compact linear antenna array; A design of highly stable and low-power SRAM cell; Optimal design of 2.4 GHz CMOS LNA using PSO with aging leader and challenger; Input-output fault diagnosis in robot manipulator using fuzzy LMI-tuned PI feedback linearization observer based on nonlinear intelligent ARX model; Robot path planning by LSTM network under changing environment.","Conference Review","0"
"45","2-s2.0-85053488820","10.1007/978-981-13-1501-5_65",NA,"Islam M.A.|Sharif N.H.|Tameem M.S.P.|Chowdhury S.M.M.H.|Kumu M.C.|Hossain M.F.","Daffodil International University","Bangladesh","2019","IoT-based robot with wireless and voice recognition mode","Advances in Intelligent Systems and Computing","814",NA,"743-752","Arduino|Communication|Internet|IoT|Microcontroller|Robotic arm|Sensor|Servo|Voice recognition module|Wi-Fi","© Springer Nature Singapore Pte Ltd. 2019. Nowadays, robots are playing a very important role in industry level and also out of the industry. Dependency on robots is increasing for their fast and reliable working speed and accuracy. Considering that, the demand of robots is increasing every day. This research was conducted focusing on the necessity of robots in our daily life. This paper proposes a system where a robot can be controlled in different ways like voice, wireless, and full automatic mode. The prototype was built and tested. The robot prototype will be able to receive voice commands from short distance. In case of long-distance communication, the user will be able to connect through the Internet using IoT.","Conference Paper","0"
"46","2-s2.0-85054153566","10.1007/978-3-030-00329-6_20",NA,"Varrasi S.|Lucas A.|Soranzo A.|McNamara J.|Di Nuovo A.","Sheffield Hallam University|IBM United Kingdom Limited","United Kingdom","2019","IBM cloud services enhance automatic cognitive assessment via human-robot interaction","Mechanisms and Machine Science","65",NA,"169-176",NA,"© Springer Nature Switzerland AG 2019. Thanks to recent developments in artificial intelligence and social robotics, Human-Robot Interaction (HRI) can be used as a non-invasive screening tool for the assessment of cognitive decline. In this scenario, the robot manages the assessment by providing the instructions to the patient, registering his/her answers and objectively calculating the final score. This service can help to save time and reach a wider population. From the technical point of view, a challenge is to achieve a highly reliable speech and visual recognition as required for a valid scoring of cognitive performance that can support the assessment from a professional. In this article, we evaluate a system for cognitive assessment that makes use of the IBM AI Cloud services embodied in one of the most popular platforms for social robotics: the SoftBank Pepper. Results of a pilot study with 16 human participants shows that IBM Cloud services for speech and visual recognition can improve the system performance in comparison with standard interfaces. Importantly, the improvement allows achieving a significant correlation with one of the most used paper-and-pencil tests and, therefore, the study demonstrates the validity of the robotic approach for cognitive assessment.","Chapter","0"
"47","2-s2.0-85054293836","10.1007/978-3-030-00232-9_7",NA,"Zafar Z.|Paplu S.H.|Berns K.","Technische Universität Kaiserslautern","Germany","2019","Real-Time recognition of extroversion-introversion trait in context of human-robot interaction","Mechanisms and Machine Science","67",NA,"63-70","Human personality|Human-robot interaction|Nonverbal cues","© Springer Nature Switzerland AG 2019. Human personality has always been an esoteric topic in the field of social science. The five-factor model dealing personality traits plays a critical role in the field of communication studies, psychology and philosophy. Judging or recognizing personality trait is undoubtedly a cognitive aspect which requires intelligence. This judgmental process is extremely fuzzy as there are so many facets ingrained in every human. Interestingly, there is no quantitative standard to judge the severity of each facet. Intuitive and perceptive skills do the trick for the person judging personality of another person. In this work, an approach has been presented that uses only nonverbal cues to recognize extroversion-introversion personality. Major facets, e.g., human posture, facial expression, speech duration, rapid body movements, etc. are considered to recognize extroversion-introversion trait. A job interview scenario has been created in which a robot interacts with a candidate and judges his personality trait. Experimental studies validate the approach which is a promising basis for the development of computing approaches capable of predicting a specific personality trait.","Chapter","0"
"48","2-s2.0-85054336799","10.1007/978-981-10-8672-4_25",NA,"Moshayedi A.J.|Agda A.M.|Arabzadeh M.","Islamic Azad University, Isfahan Branch|Department of Medical Engineering","Iran","2019","Designing and implementation a simple algorithm considering the maximum audio frequency of Persian vocabulary in order to robot speech control based on Arduino","Lecture Notes in Electrical Engineering","480",NA,"331-346","Arduino|Audio processing|Audio signal|Hearing system|Sound processing|Speech control|Speech processing","© Springer Nature Singapore Pte Ltd 2019. Based on the definition, speech process is known as the audio signals Conversion process as an input for checking systems by computer algorithms. The importance of this field is for many applications such as aerospace. Automatic translation, providing news texts from lectures, home intelligent automation, Computer games. Serving the blinds and low—power people, collecting and organizing different information resources like books, websites and also facilitate and expedite in educational services. This study has done with the purpose of introducing and testing a simple algorithm for Persian vocabulary and implementation of a robot structure based on the Arduino. The important point of this study is using audio signals in Persian language which has a history less than two decades. Although some attempts such as Nevisa and speech to text conversion software has been made, but based on researcher’s investigation, there was not found a controllable product based on Persian vocabulary. Creating relevant response with the highest audio frequency based on three Persian terms, “right”, “left” and “straight” is the purpose which has considered in this article. In this article, Sound is received by the microphone and through Matlab software by getting and processing maximum received signal frequency, the higher frequency of receiving sound will be determined and transferred to the Arduino board existence on robot through serial communication which leads to a movement reaction based on the definition of this number by a robot. The way of sampling is totally free considering the kind of microphone and distance, and results of this study in the review of three words, “left”, “right” and “straight” is an indicator of the efficiency of the suggested algorithm with the success rate of 73 and 24% failure at the first repetition and with the success rate of 86 and 14% failure at the second repetition.","Chapter","0"
"49","2-s2.0-85055055908","10.1007/978-3-030-01887-0_66",NA,"Roa M.A.|Henze B.|Ott C.","Deutsches Zentrum fur Luft- Und Raumfahrt","Germany","2019","Model-based posture control for a torque-controlled humanoid robot","Biosystems and Biorobotics","22",NA,"344-347",NA,"© Springer Nature Switzerland AG 2019. This talk presents an overview of the development of a full-body model-based passivity approach for posture control of a humanoid robot. The controller exploits passivity properties to provide suitable control inputs for the humanoid robot without requiring a solution to the inverse kinematics problem of the full kinematic chain. The controller has been validated in numerous experiments using the torque-controlled humanoid robot TORO, developed at the German Aerospace Center (DLR).","Chapter","0"
"50","2-s2.0-85055712662","10.1002/nau.23811","30375062","Yoshida M.|Matsunaga A.|Igawa Y.|Fujimura T.|Shinoda Y.|Aizawa N.|Sato Y.|Kume H.|Homma Y.|Haga N.|Sanada H.","Graduate School of Medicine and Faculty of Medicine, The University of Tokyo|University of Tokyo Hospital|Japanese Red Cross Medical Center","Japan","2019","May perioperative ultrasound-guided pelvic floor muscle training promote early recovery of urinary continence after robot-assisted radical prostatectomy?","Neurourology and Urodynamics","38","1","158-164","biofeedback|pelvic floor muscle training|prostate cancer|transperineal|ultrasound","© 2018 Wiley Periodicals, Inc. Aims: The efficacy of perioperative pelvic floor muscle training (PFMT) for continence recovery after robot-assisted radical prostatectomy (RARP) remains unclear. Visualization of the bladder neck and urethra using transperineal ultrasound (US) may promote self-recognition of urethral closure during PFM contraction. This study aimed to examine whether transperineal US-guided PFMT promotes early recovery of post-RARP incontinence. Methods: This prospective cohort study included 116 men undergoing RARP. All men were offered to undergo transperineal US-guided PFMT, and 36 men agreed. The protocol consisted of biofeedback PFMT using transperineal US before RARP and 1-month after RARP with verbal instruction of PFMT immediately after urethral catheter removal. The remaining 80 patients received verbal instruction for PFMT alone. Continence recovery was defined as the number of days requiring a small pad (20 g) per day by self-report. Results: No differences were observed in demographic or peri-operative parameters between the two groups except the longer operative time in the US-guided PFMT group. The mean time until continence recovery was significantly shorter in the US-guided PFMT group (75.6 ± 100.0 days) than in the verbal-PFMT group (121.8 ± 132.0 days, P = 0.037). Continence recovery rates within 30 days were 52.8% (19/36) and 35.4% (28/80) in the US-guided PFMT and verbal-PFMT groups, respectively (P = 0.081). US-guided PFMT was associated with better postoperative continence status (adjusted hazard ratio [95% confidence interval]: 0.550 [0.336-0.900], P = 0.017). Conclusions: The results showed that transperineal US-guided PFMT perioperatively was associated with early recovery of urinary continence after RARP.","Article","0"
"51","2-s2.0-85055807839","10.1007/978-3-030-02053-8_60",NA,"Jacquet B.|Masson O.|Jamet F.|Baratgin J.","P-A-R-I-S Association|Universite Paris 8 Vincennes-St Denis|Universite de Cergy-Pontoise|Institut Jean Nicod","France","2019","On the Lack of Pragmatic Processing in Artificial Conversational Agents","Advances in Intelligent Systems and Computing","876",NA,"394-399","Cognition|Conversations|Natural language processing|Pragmatics|Social artificial agents","© 2019, Springer Nature Switzerland AG. With the increasing demand for automated agents able to communicate with humans, a lot of progress has been made in the field of artificial intelligence in order to produce conversational agents able to sustain open or topic-restricted conversations. Still, they remain far from the capacity of interaction displayed by humans. This article highlights the challenges still faced in artificial social interaction regarding the contextualization of utterances within a conversation, either in chatbots or in more complex social robots, through processing of the pragmatic clues of conversations, using current knowledge in psychology and linguistics. It also suggests a number of points of interest for the development of artificial agents aimed at improving their communication with humans, the relevance of their utterances, and the relationship with the people interacting with them. We believe that in order to be recognized as a social agent, an artificial agent must follow similar rules humans follow themselves when conversing with each other.","Conference Paper","1"
"52","2-s2.0-85055808991","10.1007/978-3-030-02053-8_80",NA,"Silva V.|Soares F.|Esteves J.S.|Pereira A.P.","Universidade do Minho","Portugal","2019","PlayCube: Designing a Tangible Playware Module for Human-Robot Interaction","Advances in Intelligent Systems and Computing","876",NA,"527-533","Human-computer interaction|Playware|Tangible interfaces","© 2019, Springer Nature Switzerland AG. In general, humans can express their intents effortlessly. On the contrary, individuals with Autism Spectrum Disorder (ASD) present impairments in this area. Researchers are employing different technological strategies in order to improve the emotion recognition skills of individuals with ASD. Among those technological solutions, the use of Objects based on Playware Technology (OPT) in context of serious games is getting increasing attention. Following this trend, the present work proposes the development of an OPT module to be used as an add-on to the human-robot interaction with children with ASD in emotion recognition activities. To evaluate the proposed approach, usability tests with typically developing children in a school environment were conducted. Overall, the different evaluations allow estimating how the children interacted with the OPT.","Conference Paper","0"
"53","2-s2.0-85055838195","10.1007/978-3-030-02053-8_159",NA,"Chapman S.|Kirks T.|Jost J.","Fraunhofer Institute for Material Flow and Logistics IML","Germany","2019","Study on Interaction Modalities Between Humans and CPS in Sociotechnical Systems","Advances in Intelligent Systems and Computing","876",NA,"1044-1050","Human factors|Human-Robot-Interaction|User experience","© 2019, Springer Nature Switzerland AG. Nowadays, digitization and automation in manufacturing and logistics facilities lead to cyber-physical systems (CPS) which are interacting with one another and are able to handle complex and yet very flexible processes. Still there are many tasks, which cannot be handled by these CPS, and therefore, the human worker with his cognitive abilities plays a major role in such systems. To enhance the collaboration between CPS and human workers new forms of bidirectional interaction have to be setup and evaluated. In this paper, we analyzed the interaction methods of an automated transport vehicle (ATV). The ATV can be controlled via app on smartphones or tablets, gesture recognition using sensor wristbands or speech control. To obtain information about which interaction possibility should be used we have conducted a case study in which users had to control the ATV by all three modalities. During the evaluation, we examined the feedback of the user on the Likert-scale as well as objectionable information like task fulfillment time and error rate. These results helped us to identify the right interaction modality regarding the interaction task.","Conference Paper","0"
"54","2-s2.0-85055852712","10.1007/978-3-030-02053-8_32",NA,"Beriault-Poirier A.|Prom Tep S.|Sénécal S.","imarklab|Université du Québec à Montréal|HEC Montréal","Canada","2019","Putting Chatbots to the Test: Does the User Experience Score Higher with Chatbots Than Websites?","Advances in Intelligent Systems and Computing","876",NA,"204-212","Chatbots|Online consumer behavior|User experience|Websites","© 2019, Springer Nature Switzerland AG. Chatbots are robots that simulate conversation with human users through instant messaging services. Though the technology is innovative and trendy, what kind of user experience do they provide? To answer this, we conducted an exploratory study with chatbots from different sectors of activity on Facebook Messenger. We invited ten participants to complete six precise search tasks using these chatbots and the corresponding websites from each brand. Based on observation and interviews according to a qualitative approach, our study shows that the user experience scores higher with websites than chatbots, while abandonment rate is higher with chatbots, even though they generate more positive emotions. As a conclusion, until artificial intelligence improves the technology, chatbots have some catching up to do as they are do not score higher than websites to fulfill users’ expectations.","Conference Paper","0"
"55","2-s2.0-85056758140","10.1016/j.chb.2018.08.013",NA,"Desideri L.|Ottaviani C.|Malavasi M.|di Marzio R.|Bonifacci P.","Alma Mater Studiorum Università di Bologna|Emilia Romagna's Regional Centre for Assistive Technology|Università degli Studi di Roma La Sapienza|IRCCS Fondazione Santa Lucia","Italy","2019","Emotional processes in human-robot interaction during brief cognitive testing","Computers in Human Behavior","90",NA,"331-342","Cognitive assessment|Emotion|Human-robot interaction|Non-verbal behaviour|Social robotics","© 2018 Elsevier Ltd With the rapid rise in robot presence in a variety of life domains, understanding how robots influence people's emotions during human-robot interactions is important for ensuring their acceptance in society. Mental health care, in particular, is considered the field in which robotics technology will bring the most dramatic changes in the near future. In this context, the present study sought to determine whether a brief cognitive assessment conducted by a robot elicited different interaction-related emotional processes than a traditional assessment conducted by an expert clinician. A non-clinical sample of 29 young adults (17 females; M = 24.5, SD = 2.3 years) were asked to complete two cognitive tasks twice, in counterbalanced order, once administered by an expert clinician and once by an autonomous humanoid robot. Self-reported measures of affective states and assessment of physiological arousal did not reveal any difference in emotional processes between human-human and human-robot interactions. Similarly, cognitive performances and workload did not differ across conditions. Analysis of non-verbal behaviour, however, showed that participants spent more time looking at the robot (d = 1.3) and made fewer gaze aversions (d = 1.3) in interacting with the robot than with the human examiner. We argue that, far from being a trivial ‘cosmetic change’, using a social robot in place of traditional testing could be a potential way to open up the development of a new generation of tests for brief cognitive assessment.","Article","0"
"56","2-s2.0-85057082764","10.1007/978-3-030-01054-6_58",NA,"Remarczyk M.|Narayanan P.|Mitrovic S.|Black M.","Cognizant Technology Solutions","United States","2019","Our new handshake with the robot: How our changing relationship with machines can make us more human","Advances in Intelligent Systems and Computing","868",NA,"839-851","Artificial intelligence|Automation|Human-machine|Robotics","© Springer Nature Switzerland AG 2019. Very few topics in today’s digital conversations are more en vogue than Artificial Intelligence and Robotics. However, all too often, debate is centred on the machine and not the human side of this rapidly evolving handshake. This paper introduces a framework that structures the relationship from demystifying through to designing and adopting the handshake, and explores impact on enterprises and provides a future outlook.","Conference Paper","0"
"57","2-s2.0-85057114441","10.1007/978-3-030-01054-6_59",NA,"Oliveira M.L.L.|Cerqueira J.J.F.|Filho E.F.S.","Universidade Federal da Bahia","Brazil","2019","Simulation of an artificial hearing module for an assistive robot","Advances in Intelligent Systems and Computing","868",NA,"852-865","Artificial hearing|Artificial neural networks|Cepstral coefficients|Mel Frequency Cepstral Coefficients (MFCCs)|Multilateration","© Springer Nature Switzerland AG 2019. It is proposed in this paper a simulation of an artificial hearing module for a low-cost assistive robot with the purposes of: (1) identifying the speaker; (2) determining the speaker’s location; (3) estimating what emotion is transmitted by his or her speech; and (4) determining an algorithm that links the previous tasks. Mel Frequency Cepstral Coefficients (MFCCs) are used as feature extraction technique, artificial neural networks as classifier and multilateration as technique for locating a sound source. Results from both simulated and public data sets indicate the efficiency of the proposed method.","Conference Paper","0"
"58","2-s2.0-85057194424","10.1016/j.chb.2018.08.026",NA,"Lee S.A.|Liang Y.(.","Chapman University","United States","2019","Robotic foot-in-the-door: Using sequential-request persuasive strategies in human-robot interaction","Computers in Human Behavior","90",NA,"351-356","Credibility|Foot-in-the-door|Media equation|Persuasion|Robot|Robotic foot-in-the-door|Sequential compliance gaining","© 2018 Elsevier Ltd The current study investigates the effectiveness of sequential-request strategies that robots may employ to persuade humans. Specifically, this study focuses on the foot-in-the-door technique, whereby a small request is made first and is then followed up with a larger, actual target request. Participants played a trivia game with an ostensibly autonomous robot teammate. At the end of the game, the robot asked participants to complete a series of pattern recognition tasks, either by requesting directly or by starting with a small request, then following with a larger request. The results demonstrated a strong foot-in-the-door effect, suggesting a robot's potential to persuade humans using verbal message strategies. The robot's performance or perceived credibility did not influence compliance. This robotic foot-in-the-door effect provides some important practical implications for designers and developers who aim to enhance the persuasive outcomes of human-robot interaction.","Article","0"
"59","2-s2.0-85057208170","10.1016/j.chb.2018.08.027",NA,"Edwards C.|Edwards A.|Stoll B.|Lin X.|Massey N.","Western Michigan University|Cornell University|Pennsylvania State University","United States","2019","Evaluations of an artificial intelligence instructor's voice: Social Identity Theory in human-robot interactions","Computers in Human Behavior","90",NA,"357-362","Age|AI|CASA|Credibility|HRI|Social identity theory|Social presence|Voice","© 2018 Elsevier Ltd This study employs the Computers are Social Actors (CASA) paradigm to extend the predictions of Social Identity Theory (SIT) to human-robot interaction (HRI) in the context of instructional communication. SIT posits that individuals gain a sense of personal worth from the groups with which they identify. Previous research has demonstrated that age group identification is meaningful to individuals’ self-concepts. Results demonstrated that higher age identified students rated the older A.I. voice instructor (representing an out-group member) higher for credibility and social presence and reported more motivation to learn than those students with low age identification. Implications are discussed for SIT and design features of computerized voices.","Article","0"
"60","2-s2.0-85057786065","10.1007/978-3-319-99885-5_15",NA,"Rodriguez I.|Manfré A.|Vella F.|Infantino I.|Lazkano E.","Universidad del Pais Vasco|Istituto Di Calcolo E Reti Ad Alte Prestazioni, Rende","Spain|Italy","2019","Talking with Sentiment: Adaptive Expression Generation Behavior for Social Robots","Advances in Intelligent Systems and Computing","855",NA,"209-223","Cognitive robotics|Generative adversarial network|Human-robot interaction|Social robotics","© 2019, Springer Nature Switzerland AG. This paper presents a neural-based approach for generating natural gesticulation movements for a humanoid robot enriched with other relevant social signals depending on sentiment processing. In particular, we take into account some simple head postures, voice parameters, and eyes colors as expressiveness enhancing elements. A Generative Adversarial Network (GAN) allows the proposed system to extend the variability of basic gesticulation movements while avoiding repetitive and monotonous behavior. Using sentiment analysis on the text that will be pronounced by the robot, we derive a value for emotion valence and coherently choose suitable parameters for the expressive elements. In this way, the robot has an adaptive expression generation during talking. Experiments validate the proposed approach by analyzing the contribution of all the factors to understand the naturalness perception of the robot behavior.","Conference Paper","0"
"61","2-s2.0-85057823193","10.1007/978-3-319-99885-5_14",NA,"Vega A.|Manso L.J.|Cintas R.|Núñez P.","Universidad de Extremadura|Aston University","Spain|United Kingdom","2019","Planning Human-Robot Interaction for Social Navigation in Crowded Environments","Advances in Intelligent Systems and Computing","855",NA,"195-208",NA,"© 2019, Springer Nature Switzerland AG. Navigation is one of the crucial skills autonomous robots need to perform daily tasks, and many of the rest depend on it. In this paper, we argue that this dependence goes both ways in advanced social autonomous robots. Manipulation, perception, and most importantly human-robot interaction are some of the skills in which navigation might rely on. This paper is focused on the dependence on human-robot interaction and uses two particular scenarios of growing complexity as an example: asking for collaboration to enter a room and asking for permission to navigate between two people which are talking. In the first scenario, the person physically blocks the path to the adjacent room, so it would be impossible for the robot to navigate to such room. Even though in the second scenario the people talking do not block the path to the other room, from a social point of view, interrupting an ongoing conversation without noticing is undesirable. In this paper we propose a navigation planning domain and a set of software agents which allow the robot to navigate in crowded environments in a socially acceptable way, asking for cooperation or permission when necessary. The paper provides quantitative experimental results including social navigation metrics and the results of a Likert-scale satisfaction questionnaire.","Conference Paper","0"
"62","2-s2.0-85057874949","10.1007/978-3-319-99885-5_5",NA,"González-Medina D.|Romero-González C.|García-Varea I.","Universidad de Castilla-La Mancha","Spain","2019","Combination of Semantic Localization and Conversational Skills for Assistive Robots","Advances in Intelligent Systems and Computing","855",NA,"56-69","Artificial vision|Assistive robotics|Human-robot interaction|Smart homes|Speech recognition","© 2019, Springer Nature Switzerland AG. The recognition of objects and their features is a fundamental task for social robots that could be improved with the combination of different sources of information, such as the ones provided by visual or speech understanding systems. In this paper, we present a first approach to fusion semantic localization and conversational skills for social robots which may act as assistants. Our solution is based on a mobile robot that is able to detect and recognize objects from an environment and store them in its base of knowledge to later act as an assistant for any user who is searching for any object. In the conversation the robot tries to help the user to find a specific object depending of the location and the features of the object which is looking for. The proposal has been empirically evaluated within a research lab where the robot recognizes objects in the environment and the users require, by means of speech commands, finding suitable objects that are placed in the environment.","Conference Paper","0"
"63","2-s2.0-85058179983",NA,NA,"Jonell P.|Bystedt M.|Fallgren P.|Kontogiorgos D.|Lopes J.|Malisz Z.|Mascarenhas S.|Oertel C.|Raveh E.|Shore T.","The Royal Institute of Technology (KTH)|Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento em Lisboa|Universität des Saarlandes","Sweden|Portugal|Germany","2019","Farmi: A framework for recording multi-modal interactions","LREC 2018 - 11th International Conference on Language Resources and Evaluation",NA,NA,"3969-3974","Human-robot interaction|Multimodal interaction|Multisensory processing","© LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved. In this paper we present (1) a processing architecture used to collect multi-modal sensor data, both for corpora collection and real-time processing, (2) an open-source implementation thereof and (3) a use-case where we deploy the architecture in a multi-party deception game, featuring six human players and one robot. The architecture is agnostic to the choice of hardware (e.g. microphones, cameras, etc.) and programming languages, although our implementation is mostly written in Python. In our use-case, different methods of capturing verbal and non-verbal cues from the participants were used. These were processed in real-time and used to inform the robot about the participants' deceptive behaviour. The framework is of particular interest for researchers who are interested in the collection of multi-party, richly recorded corpora and the design of conversational systems. Moreover for researchers who are interested in human-robot interaction the available modules offer the possibility to easily create both autonomous and wizard-of-Oz interactions.","Conference Paper","1"
"64","2-s2.0-85059068828",NA,NA,"NA NA","NA","NA","2019","8th International Workshop of Advanced Manufacturing and Automation, IWAMA 2018","Lecture Notes in Electrical Engineering","484",NA,"",NA,"The proceedings contain 89 papers. The special focus in this conference is on Advanced Manufacturing and Automation. The topics include: HDPS-BPSO based predictive maintenance scheduling for backlash error compensation in a machining center; influence of the length-diameter ratio and the depth of liquid pool in a bowl on separation performance of a decanter centrifuge; LSTM based prediction and time-temperature varying rate fusion for hydropower plant anomaly detection: A case study; wind turbine system modelling using bond graph method; on opportunities and limitations of additive manufacturing technology for industry 4.0 era; operator 4.0 – Emerging job categories in manufacturing; reliability analysis of centrifugal pump based on small sample data; research on horizontal vibration of traction elevator; research on real-time monitoring technology of equipment based on augmented reality; A DEMO of smart manufacturing for mass customization in a lab; Research on the relationship between sound and speed of a DC motor; review and analysis of processing principles and applications of self-healing composite materials; Scattered parts for robot bin-picking based on the universal V-REP platform; brain network analysis based on resting state functional magnetic resonance image; development of bicycle smart factory and exploration of intelligent manufacturing talents cultivation; the journey towards world class maintenance with profit loss indicator; initiating industrie 4.0 by implementing sensor management – Improving operational availability; a prediction method for the ship rust removal effect of pre-mixed abrasive jet; a review of dynamic control of the rigid-flexible macro-micro manipulators; analysis of speech enhancement algorithm in industrial noise environment.","Conference Review","0"
"65","2-s2.0-85059074341","10.1007/978-3-030-04585-2_11",NA,"Liu L.|Zhu S.|He D.|Ma Y.|Zhang X.|Huang J.|Li J.","Fujian University of Technology","China","2019","Design and realization of intelligent voice-control car based on raspberry Pi","Smart Innovation, Systems and Technologies","128",NA,"87-95","Intelligent car|Remote control|Speech recognition|Turing","© Springer Nature Switzerland AG 2019. This paper introduces an intelligent voice-control car based on Raspberry Pi. The user’s speech is recorded and uploaded to Baidu cloud to be recognized and analyzed by Baidu speech recognition technology. The intelligent car movement can be controlled according to the returned recognition results if control commands included. Meanwhile, Turing robot technology enables the car to dialogue with the users. The car can also be remotely controlled by using computers and other intelligent devices in real time. The testing shows that no matter it is on-site operation or remote operation, the car can work efficiently with high correction of speech recognition.","Conference Paper","0"
"66","2-s2.0-85059123173","10.1016/j.urology.2018.07.062","30598205","Whitaker D.L.|White W.M.","University of Alabama School of Medicine|University of Tennessee Medical Center","United States","2019","Editorial Comment","Urology","123",NA,"155-156",NA,NA,"Note","0"
"67","2-s2.0-85059660031","10.1007/978-981-13-1742-2_25",NA,"Gomez H.F.A.|Arias S.A.T.|Lozada T.E.F.|Martínez C.C.E.|Robalino F.|Castillo D.|Luz P.","Universidad Técnica de Ambato|Universidad Regional Autónoma de los Andes|Universidad Indoamerica","Ecuador","2019","Emotional strategy in the classroom based on the application of new technologies: An initial contribution","Smart Innovation, Systems and Technologies","106",NA,"251-261","Emotions|Student|Technologies","© 2019, Springer Nature Singapore Pte Ltd. The main purpose of a class is for students to generate positive emotions and to maintain them along the semester; in this way, students not only learn but also manage to retain learning in order to be applied in concrete and real facts. In this study we show a learning mechanism based on the inclusion of NTICS based on our EVA robot. The initial results showed that the level of attention in positive emotions was 100% of the class. During the course of 2 months it decrease due to the lack of updates in the technology operation introduced in the process. However, the results in the study time show a positive emotional level greater than 60% and a concentration level of 80%. All these based on speech and analysis of facial emotion. These results are primary in the inclusion of new technologies in the classroom, so we propose conclusions and recommendations that will improve our future research.","Conference Paper","0"
"68","2-s2.0-85059819618","10.2316/J.2019.206-5163",NA,"Zheng M.|Liu P.X.|Meng M.Q.H.","Beijing Jiaotong University|Carleton University|Chinese University of Hong Kong","China|Canada|Hong Kong","2019","Interpretation of human and robot emblematic gestures: Howdo they differ?","International Journal of Robotics and Automation","34","1","55-70","Emblematic gestures|Human-robot interaction|Humanoid robot|Robot gesture design|Social robotics","© 2019 E-flow Acta Press/IASTED. All rights reserved. Human gestures are often directly adopted in gesture design for humanoid robots when human-robot interaction is considered, but the interpretations of human and robot gestures can vary due to the structural difference between human and robot and due to people's perceptual difference of human and robot. In this paper, we investigate how people interpret human and robot gestures differently using emblematic gestures, a type of gesture with specific verbal translations. Nine commonly used emblems performed by a human were evaluated by 32 participants, and the nine emblems performed by a NAO humanoid robot were evaluated by another 32 participants. We compared participants' interpretations of the human and robot emblems systematically. The results indicate that the multimessage gesture and multigesture message characteristics of human emblems also exist in robot emblems. However, participants' recognition rates of robot emblems are lower than those of human emblems in many cases. The human and robot emblems can also vary in interpretations and corresponding percentages. Some interpretations of human emblems do not appear in robot emblems and vice versa; even when the interpretations of human and robot emblems are consistent, the corresponding percentage of each interpretation can vary between human and robot. Besides, participants' recognition of robot emblems can be affected by their familiarity with humanoids.","Article","0"
"69","2-s2.0-85059822330","10.1007/978-3-030-05594-3_16",NA,"Kotov A.|Arinkin N.|Zaidelman L.|Zinina A.","National Research Centre ""Kurchatov Institute""|Russian State University for the Humanities","Russian Federation","2019","Linguistic approaches to robotics: From text analysis to the synthesis of behavior","Communications in Computer and Information Science","943",NA,"207-214","Emotional agents|Robot companions|Semantic processing|Syntactic parsers","© Springer Nature Switzerland AG 2019. We examine the problem of “understanding robots” and design an F–2 emotional robot to “understand” speech and to support human-like behavior. The suggested system is an applied implementation of the theoretical concept of robotic information flow, suggested by M. Minsky (“proto-specialists”) and A. Sloman (CogAff). This system works with real world input – natural texts, speech sound – and produces natural behavioral output – speech, gestures and facial expressions. Unlike other chatbots, the system relies on semantic representation and operates with a set of d-scripts (equivalents to proto-specialists), extracted from advertising and mass media texts as a classification of basic emotional patterns. The process of “understanding” is modelled as the selection of a relevant d-script for the incoming utterance.","Conference Paper","0"
"70","2-s2.0-85059832489","10.1002/ad.2398",NA,"Young L.","NA","NA","2019","‘I'm a Cloud of Infinitesimal Data Computation’ When Machines Talk Back: An interview with Deborah Harrison, one of the personality designers of Microsoft's Cortana AI","Architectural Design","89","1","112-117","2001: A Space Odyssey|343 team|Alexa|Amazon|Apple|Bisojo|Cortana|Crypton Future Media|Deborah Harrison|Dr Kohei Ogawa|DX7 keyboard synthesiser|ERATO ISHIGURO Symbiotic Human–Robot Interaction Project|ERICA (ERato Intelligent Conversational Android)|Geminoid HI-4|Google|Halo|Hatsune Miku|Her|Japanese AI pop star|KEI|Microsoft|Otaku features|Post-Anthropocene|Professor Hiroshi Ishiguro's Intelligent Robotics Laboratory|Scarlett Johannson|Siri|Spike Jonze|Spotify|Stanley Kubrick|University of Osaka|Vocaloid|Wataru Sasaki|Xbox","Copyright © 2019 John Wiley  &  Sons, Ltd. How is the personality of an artificial intelligence crafted, and what are the issues at stake? As one of the original architects of Microsoft's digital assistant Cortana's personality, Deborah Harrison knows the process inside out. In an interview with Guest-Editor Liam Young, she examines the questions that creating this AI raised in terms of gender, culture and ethics, and considers the future of machine interactions. Wataru Sasaki, lead developer of the software behind the AI pop star Hatsune Miku, and android engineer Kohei Ogawa also join the discussion.","Article","0"
"71","2-s2.0-85059871270","10.1002/ad.2400",NA,"Young L.","NA","NA","2019","Not For Us: Squatting the Ruins of Our Robot Utopia: An Interview with Paul Inglis, Supervising Art Director of Blade Runner 2049","Architectural Design","89","1","126-135","2049|Andreas Gursky|Bangladesh|Blade Runner|China|Chittagong|Denis Villeneuve|e-waste fields|Edward Burtynsky|Ghana|Harrison Ford|India|Lieutenant Joshi|Officer K|protein-farm|Ridley Scott|Sapper Morton|Shinjuku|Tokyo|Victor Martinez|Wallace Corporatino|‘spinner’","Copyright © 2019 John Wiley  &  Sons, Ltd. A lonely world where everyone who can has fled, and those who remain dwell amid the remnants of environments designed for machines rather than people: this is the vision of Earth in three decades’ time that is the backdrop to the 2017 film Blade Runner 2049. Guest-Editor Liam Young talks to supervising art director Paul Inglis and presents the work of concept artist Victor Martinez to explore how the team conceived this dystopian landscape.","Article","0"
"72","2-s2.0-85059879494",NA,NA,"Guntakandla N.|Nielsen R.","University of North Texas","United States","2019","Annotating reflections for health behavior change therapy","LREC 2018 - 11th International Conference on Language Resources and Evaluation",NA,NA,"4002-4007","Behavioral codes|Companion robot conversations|Motivational Interview|Reflections|Social-therapeutic agents|Virtual health agents","© LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved. We present our work on annotating reflections, an essential counselor behavioral code in motivational interviewing for psychotherapy on conversations that are a combination of casual and therapeutic dialogue. We annotated all the therapists' utterances from ten transcripts spanning more than five hours of conversation with Complex Reflection, Simple Reflection, or No Reflection. We also provide insights into corpus quality and code distributions. The corpus that we constructed and annotated in this effort is a vital resource for automated health behavior change therapy via a dialogue system. As the on-going work, additional conversations are being annotated at least by one annotator.","Conference Paper","0"
"73","2-s2.0-85059884916","10.1007/978-3-030-01370-7_47",NA,"Kawasaki Y.|Yorozu A.|Takahashi M.","Keio University","Japan","2019","Multimodal path planning using potential field for human–robot interaction","Advances in Intelligent Systems and Computing","867",NA,"597-609","Human–robot interaction|Multimodal path planning|Potential field","© 2019, Springer Nature Switzerland AG. In a human–robot interaction, a robot must move to a position where the robot can obtain precise information of people, such as positions, postures, and voice. This is because the accuracy of human recognition depends on the positional relation between the person and robot. In addition, the robot should choose what sensor data needs to be focused on during the task that involves the interaction. Therefore, we should change a path approaching the people to improve human recognition accuracy for ease of performing the task. Accordingly, we need to design a path-planning method considering sensor characteristics, human recognition accuracy, and the task contents simultaneously. Although some previous studies proposed path-planning methods considering sensor characteristics, they did not consider the task and the human recognition accuracy, which was important for practical application. Consequently, we present a path-planning method considering the multimodal information which fusion the task contents and the human recognition accuracy simultaneously.","Conference Paper","0"
"74","2-s2.0-85059901348",NA,NA,"Tufi<U+0219> D.|Cristea D.","Academia Româna|Universitatea Alexandru Ioan Cuza","|Romania","2019","A bird's-eye view of language processing projects at the Romanian academy","LREC 2018 - 11th International Conference on Language Resources and Evaluation",NA,NA,"2445-2451","Contemporary language reference corpus|Intelligent character recognition|Medical records archive|Natural language dialogue systems|Speech corpus","© LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved. This article gives a general overview of five AI language-related projects that address contemporary Romanian language, in both textual and speech form, language related applications, as well as collections of old historic documents and medical archives. Namely, these projects deal with: the creation of a contemporary Romanian language text and speech corpus, resources and technologies for developing human-machine interfaces in spoken Romanian, digitization and transcription of old Romanian language documents drafted in Cyrillic into the modern Latin alphabet, digitization of the oldest archive of diabetes medical records and dialogue systems with personal robots and autonomous vehicles. The technologies involved for attaining the objectives range from image processing (intelligent character recognition for hand-writing and old Romanian documents) to natural language and speech processing techniques (corpus compiling and documentation, multi-level processing, transliteration of different old scripts into modern Romanian, command language processing, various levels of speech-text alignments, ASR, TTS, keyword spotting, etc.). Some of these projects are approaching the end, others have just started and others are about to start. All the reported projects are national ones, less documented than the international projects we are/were engaged in, and involve large teams of experts and master/PhD students from computer science, mathematics, linguistics, philology and library sciences.","Conference Paper","0"
"75","2-s2.0-85059903923",NA,NA,"Gross S.|Hirschmanner M.|Krenn B.|Neubarth F.|Zillich M.","Austrian Research Institute for Artificial Intelligence|Technische Universitat Wien","Austria","2019","Action verb corpus","LREC 2018 - 11th International Conference on Language Resources and Evaluation",NA,NA,"2147-2151","Corpus|Language modelling|Multimodality","© LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved. The Action Verb Corpus comprises multimodal data of 12 humans conducting in total 390 simple actions (TAKE, PUT, and PUSH). Recorded are audio, video and motion data while participants perform an action and describe what they do. The dataset is annotated with the following information: orthographic transcriptions of utterances, part-of-speech tags, lemmata, information which object is currently moved, information whether a hand touches an object, information whether an object touches the ground/table. Transcription, and information whether an object is in contact with a hand and which object moves where to were manually annotated, the rest was automatically annotated and manually corrected. In addition to the dataset, we present an algorithm for the challenging task of segmenting the stream of words into utterances, segmenting the visual input into a series of actions, and then aligning visual action information and speech. This kind of modality rich data is particularly important for crossmodal and cross-situational word-object and word-action learning in human-robot interactions, and is comparable to parent-toddler communication in early stages of child language acquisition.","Conference Paper","0"
"76","2-s2.0-85059906682",NA,NA,"Traum D.|Henry C.|Lukin S.|Artstein R.|Gervitz F.|Pollard K.A.|Bonial C.|Lei S.|Voss C.R.|Marge M.|Hayes C.J.|Hill S.G.","University of Southern California|U.S. Army Research Laboratory|Tufts University","United States","2019","Dialogue structure annotation for multi-floor interaction","LREC 2018 - 11th International Conference on Language Resources and Evaluation",NA,NA,"104-111","Dialogue structure annotation|Human-robot interaction|Multiparty dialogue","© LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved. We present an annotation scheme for meso-level dialogue structure, specifically designed for multi-floor dialogue. The scheme includes a transaction unit that clusters utterances from multiple participants and floors into units according to realization of an initiator's intent, and relations between individual utterances within the unit. We apply this scheme to annotate a corpus of multi-floor human-robot interaction dialogues. We examine the patterns of structure observed in these dialogues and present inter-annotator statistics and relative frequencies of types of relations and transaction units. Finally, some example applications of these annotations are introduced.","Conference Paper","0"
"77","2-s2.0-85059907580","10.1007/978-3-030-01370-7_45",NA,"Takahashi S.|Mizuuchi I.","Tokyo University of Agriculture and Technology","Japan","2019","Operating a robot by nonverbal voice expressed with acoustic features","Advances in Intelligent Systems and Computing","867",NA,"573-584",NA,"© 2019, Springer Nature Switzerland AG. This paper proposes methods for operating a robot by nonverbal voice. These methods enable operators to operate multi-degrees of freedom simultaneously and operate a robot intuitively by nonverbal voice by associating the nonverbal voice, tongue position and the coordinate of the robot’s hand. The voice is defined by formants or Mel Frequency Cepstral Coefficients (MFCC). Formants and MFCC are acoustic features and they show the characteristics of the vocal tract such as the mouth and the tongue. We propose two methods. One is the method in which voice expressed with overlapped formants ranges are used to change variable about robots’ operation. This method enables operators to operate multi-degrees of freedom simultaneously by nonverbal voice. The other is the method that operators tongue positions are distinguished by nonverbal voice. These tongue positions correspond to the coordinate of the robot’s hand and it enables the operators to operate a robot intuitively. We found the feasibility of the methods through experiments of simple tasks. These methods can realize operating a robot intuitively in continuous values by voice and can be utilized for user-friendly system.","Conference Paper","0"
"78","2-s2.0-85059910694",NA,NA,"Artstein R.|Boberg J.|Gainer A.|Gratch J.|Johnson E.|Leuski A.|Lucas G.M.|Traum D.","Institute for Creative Technologies","United States","2019","The Niki and Julie corpus: Collaborative multimodal dialogues between humans, robots, and virtual agents","LREC 2018 - 11th International Conference on Language Resources and Evaluation",NA,NA,"2928-2932","Collaborative problem-solving|Dialogue|Human-robot interaction|Social influence","© LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved. The Niki and Julie corpus contains more than 600 dialogues between human participants and a human-controlled robot or virtual agent, engaged in a series of collaborative item-ranking tasks designed to measure influence. Some of the dialogues contain deliberate conversational errors by the robot, designed to simulate the kinds of conversational breakdown that are typical of present-day automated agents. Data collected include audio and video recordings, the results of the ranking tasks, and questionnaire responses; some of the recordings have been transcribed and annotated for verbal and nonverbal feedback. The corpus has been used to study influence and grounding in dialogue. All the dialogues are in American English.","Conference Paper","0"
"79","2-s2.0-85059913495",NA,NA,"Kraus M.|Kraus J.|Baumann M.|Minker W.","Universität Ulm","Germany","2019","Effects of gender stereotypes on trust and likability in spoken human-robot interaction","LREC 2018 - 11th International Conference on Language Resources and Evaluation",NA,NA,"112-118","Gender Stereotypes|Human-Robot Interaction|Trust","© LREC 2018 - 11th International Conference on Language Resources and Evaluation. All rights reserved. As robots enter more and more areas of everyday life, it becomes necessary for them to interact in an understandable and trustworthy way. In many regards this requires a human-like interaction pattern. This research investigates the influence of gender stereotypes on trust and likability of humanoid robots. In this endeavor, explicit (name and voice) and implicit gender (personality) of robots have been manipulated along with the stereotypicality of a task. 40 participants interacted with a NAO robot to gain feedback on a task they were working on and rated the perception of the robot cooperation partner. While no gender stereotypes were found for the explicit gender, implicit gender showed a strong effect on trust and likability in the stereotypical male task. Participants trusted the male robot more and rated it as more reliable and competent than the female personality robot, while the female robot was perceived as more likable. These findings indicate that for gender stereotypes in robot interaction a differentiation between explicit and implicit stereotypical features have to be drawn and that the task context needs consideration. Future research may look into situational variables that drive stereotypification in human-robot interaction.","Conference Paper","0"
"80","2-s2.0-85059933304","10.1007/978-3-030-11051-2_121",NA,"Fank J.|Lienkamp M.","Technical University of Munich","Germany","2019","“i’m your personal co-driver—how can i assist you?” assessing the potential of personal assistants for truck drivers","Advances in Intelligent Systems and Computing","903",NA,"795-800","Hedonic attributes|Online survey|Personal assistant|Pragmatic attributes|Social robots|Truck drivers|Virtual agents","© 2019, Springer Nature Switzerland AG. User acceptance is widely recognized as a major factor for predicting the intention to use a technical device. As such, personal assistants for truck drivers pose significant design and function challenges. Long, solitary hours and comprehensive interaction with the vehicle open various possibilities for an assistant’s service. Including drivers’ requirements early in development is hence beneficial. This paper describes the result of an online survey intended to assess truck drivers’ attitudes toward personal assistants in the truck cabin. Its authors investigate the potential and the predicted acceptance of personal conversation, virtual, or robotic agents as interaction partners. They furthermore analyze how hedonic and pragmatic attributes affect the intention to use these personal assistants.","Conference Paper","0"
"81","2-s2.0-85060258622","10.1007/978-3-030-00665-5_170",NA,"Shelke R.|Nemade M.","Pacific Academy of Higher Education and Research University|K. J. Somaiya Institute of Engineering &amp; Information Technology","India","2019","Performance evaluation of audio watermarking algorithms using DWT and LFSR","Lecture Notes in Computational Vision and Biomechanics","30",NA,"1857-1864","Audio watermarking algorithm|LFSR|Signal processing attacks","© Springer Nature Switzerland AG 2019. Advancement in Internet technology has compelled encryption, copyright protection and authentication of audios, videos, images, and documents. Watermarking techniques have been widely employed in copyright protection and authentications of digital media. Digital image watermarking techniques are extensively explored and found suitable for copyright protection of images, whereas audio watermarking techniques are less studied and need extensive research due to superior hearing ability of the human beings. In this paper, we present audio watermarking technique using discrete wavelet transform (DWT) and linear feedback shift registers (LFSR). Watermark signal is obtained using LFSR technique before embedding into audio signal. Dispersion of maximum power spectral density property of LFSR is explored that makes it suitable as scramblers. LFSR does not require secret key for scrambling and descrambling of watermark signal. Sequences were embedded into the DWT coefficients of the audio signal. Experimental simulations were performed to evaluate the performance of the audio watermarking technique. Audio watermarking finds applications in the area of ownership protection, tamper detection and authentication of music, military communication, voice-activated machines, and robots.","Chapter","0"
"82","2-s2.0-85060597901","10.1017/ATSIP.2018.32",NA,"Ishi C.T.|Minato T.|Ishiguro H.","Advanced Telecommunications Research Institute International (ATR)","Japan","2019","Analysis and generation of laughter motions, and evaluation in an android robot","APSIPA Transactions on Signal and Information Processing",NA,NA,"","Emotion expression|Human-robot interaction|Laughter|Motion generation|Non-verbal information","Copyright © The Authors, 2019 This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited. Laughter commonly occurs in daily interactions, and is not only simply related to funny situations, but also to expressing some type of attitudes, having important social functions in communication. The background of the present work is to generate natural motions in a humanoid robot, so that miscommunication might be caused if there is mismatching between audio and visual modalities, especially in laughter events. In the present work, we used a multimodal dialogue database, and analyzed facial, head, and body motion during laughing speech. Based on the analysis results of human behaviors during laughing speech, we proposed a motion generation method given the speech signal and the laughing speech intervals. Subjective experiments were conducted using our android robot by generating five different motion types, considering several modalities. Evaluation results showed the effectiveness of controlling different parts of the face, head, and upper body (eyelid narrowing, lip corner/cheek raising, eye blinking, head motion, and upper body motion control).","Article in Press","0"
"83","2-s2.0-85060642987","10.1177/0020294018819552",NA,"Lin Y.|Zhou H.|Chen M.|Min H.","Wuhan University of Science and Technology|Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System","China","2019","Automatic sorting system for industrial robot with 3D visual perception and natural language interaction","Measurement and Control (United Kingdom)","52","1-2","100-115","automatic programming|Automatic sorting system|human–robot interaction|human–robot–environment interaction|rule-scene matching algorithm","© The Author(s) 2019. With the development of the technologies such as computer vision, natural language interaction, process control technology, and sensor technology, the discussion of automatic sorting system of robot hits a hotspot in the robot community. This paper presents an automatic sorting system for industrial robot with the technologies of 3D visual perception, natural language interaction and automatic programming. Therein, a ‘rule-scene’ matching and interaction algorithm is proposed to combine all these modules together. By utilising our algorithm, robot can interact with human beings according to the real-time actual three-dimensional scene information and can guide users to give correct rules through speech when the rule is invalid. After getting the correct rules, robot can sort the object automatically using the automatic programming and execution algorithm designed in this study. In the experimental section, the designed system is applied to a fruit-sorting scene, which proves the effectiveness and practicability of the system.","Article","0"
"84","2-s2.0-85060685656","10.1017/ATSIP.2018.32",NA,"Ishi C.T.|Minato T.|Ishiguro H.","Advanced Telecommunications Research Institute International (ATR)","Japan","2019","Analysis and generation of laughter motions, and evaluation in an android robot","APSIPA Transactions on Signal and Information Processing","8",NA,"","Emotion expression|Human-robot interaction|Laughter|Motion generation|Non-verbal information","Copyright © The Authors, 2019a This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited. Laughter commonly occurs in daily interactions, and is not only simply related to funny situations, but also to expressing some type of attitudes, having important social functions in communication. The background of the present work is to generate natural motions in a humanoid robot, so that miscommunication might be caused if there is mismatching between audio and visual modalities, especially in laughter events. In the present work, we used a multimodal dialogue database, and analyzed facial, head, and body motion during laughing speech. Based on the analysis results of human behaviors during laughing speech, we proposed a motion generation method given the speech signal and the laughing speech intervals. Subjective experiments were conducted using our android robot by generating five different motion types, considering several modalities. Evaluation results showed the effectiveness of controlling different parts of the face, head, and upper body (eyelid narrowing, lip corner/cheek raising, eye blinking, head motion, and upper body motion control).","Article","0"
"85","2-s2.0-85061032936","10.1007/s10015-019-00529-x",NA,"Mohd Anuardi M.N.A.|Yamazaki A.K.","Shibaura Institute of Technology","Japan","2019","Effects of emotionally induced language sounds on brain activations for communication","Artificial Life and Robotics",NA,NA,"","Brain function|Communication|Emotion|Language area|Language sounds|Working memory","© 2019, International Society of Artificial Life and Robotics (ISAROB). Emotions play an important role in human communication. We conducted a study to identify the effect of emotions in language sounds in terms of brain functions. The sounds of Japanese sentences spoken with and without emotion were reversed to eliminate their semantic influence on the subjects’ emotional perception. Three sets of sentences with non-emotional and happy, sad, and angry emotional tones were recorded and reversed. The brain activities of 20 native Japanese speakers in their twenties were monitored by near-infrared spectroscopy (NIRS) while they listened to the reversed Japanese sounds with and without the emotions. Our analysis of the experimental results demonstrated that almost all the brain areas monitored by the NIRS probes were activated more when the subjects listened to emotional language sounds than to non-emotional sounds. In particular, the frontopolar cortex area, which is associated with short-term memory, was significantly activated. Since short-term memory is known to provide important information for communication, these results suggest that emotional aspects of language sounds are essential for successful communication and thus should be implemented in human–robot communication systems.","Article","0"
"86","2-s2.0-85061080042","10.1007/978-3-030-02804-6_5",NA,"Yu X.|Lv S.|Mao K.|Wang A.|Han L.|Gao C.|Yang G.","State Grid Shandong Electric Power Maintenance Company|Shandong Luneng Intelligence Technology Co., Ltd.","China","2019","Design and implement of speech interaction with indoor substation inspection robot","Advances in Intelligent Systems and Computing","885",NA,"40-46","Human–machine interaction|Speech application programming interface|Speech recognition|Substation inspection robot","© Springer Nature Switzerland AG 2019. Regular inspecting electrical equipments in substation is an important task, where substation inspection robots are developed and used. After analyzing the application requirement of speech interaction with substation inspection robot, a speech interaction system has been developed based on Microsoft’s speech application programming interface. Then the design ideas of the system and some key steps, including speech recognition, text-to-speech, grammar creation and maintenance, are thoroughly stated. How to further improve the recognition rate is discussed in the end.","Conference Paper","0"
"87","2-s2.0-85061087840","10.1007/978-3-030-02804-6_10",NA,"Fu C.|Sun Z.|Tian K.|Dong M.|Yang G.|Li J.|Zhang C.|Shao G.","Shandong Luneng Intelligence Technology Co., Ltd.","China","2019","Study of substation inspection robot voice recognition algorithm based on wavelet transform","Advances in Intelligent Systems and Computing","885",NA,"81-87","Inspection robot|LBG algorithm|Voice recognition|VQ algorithm|Wavelet decompositions","© Springer Nature Switzerland AG 2019. A kind of substation inspection robot device voice recognition algorithm is proposed based on wavelet decompositions, which can recognize the running state of substation device. Firstly, the sample library was collected by robot pickup. Secondly, the sub-band energy of samples was extracted by using wavelet decomposition; the sample code book was billed by using VQ algorithm and LBG algorithm. Finally, using the codebook to identify sound samples, which can judge the running state of the equipment. The experimental results show that, the algorithm can effectively identify the sound equipment and high accuracy. The requirement of the substation inspection robot for detecting device is satisfied.","Conference Paper","0"
"88","2-s2.0-85061096947","10.1109/ACCESS.2019.2891838",NA,"Zeng M.|Xiao N.","South China University of Technology","China","2019","Effective combination of DenseNet and BiLSTM for keyword spotting","IEEE Access","7",NA,"10767-10775","attention mechanism|DenseNet|long short-term memory|speech recognition|spotting","© 2013 IEEE. Keyword spotting (KWS) is a major component of human-computer interaction for smart on-device terminals and service robots, the purpose of which is to maximize the detection accuracy while keeping footprint size small. In this paper, based on the powerful ability of DenseNet on extracting local feature-maps, we propose a new network architecture (DenseNet-BiLSTM) for KWS. In our DenseNet-BiLSTM, the DenseNet is primarily applied to obtain local features, while the BiLSTM is used to grab time series features. In general, the DenseNet is used in computer vision tasks, and it may corrupt contextual information for speech audios. In order to make DenseNet suitable for KWS, we propose a variant DenseNet, called DenseNet-Speech, which removes the pool on the time dimension in transition layers to preserve speech time series information. In addition, our DenseNet-Speech uses less dense blocks and filters to keep the model small, thereby reducing time consumption for mobile devices. The experimental results show that feature-maps from DenseNet-Speech maintain time series information well. Our method outperforms the state-of-the-art methods in terms of accuracy on Google Speech Commands dataset. DenseNet-BiLSTM is able to achieve the accuracy of 96.6% for the 20-commands recognition task with 223K trainable parameters.","Article","0"
"89","2-s2.0-85061099315","10.1007/978-3-030-02804-6_6",NA,"Han L.|Gao C.|Zhang S.|Li D.|Sun Z.|Yang G.|Li J.|Zhang C.|Shao G.","Shandong Luneng Intelligence Technology Co., Ltd.","China","2019","Speech recognition algorithm of substation inspection robot based on improved DTW","Advances in Intelligent Systems and Computing","885",NA,"47-54","DTW|Endpoint detection|Inspection robot|MFCC|Speech recognition","© Springer Nature Switzerland AG 2019. The voice recognition algorithm is proposed in this paper which can control the substation inspection robot by voice, so the voice-controlled robot is realized. Combined the voice controlled command collected by robot’s pickup device and the basic principle of speech recognition, the intelligent mobile robot can make the corresponding action according to the instructions and complete automatic detection and information query. Firstly, the voice instruction was collected by using robot and transmitted system based to construct the sample database. Secondly, the sample database was analyzed and MFCC feature of speech sample was extracted. Finally, the template matching of voice parameters was achieved by using the improved DTW as the matching algorithm, and the recognition result was transmitted the robot system to control the robot’s action. The experimental results show that the algorithm can quickly recognize and extract the voice command, so it can improve the recognition accuracy and real-time control the inspection robot.","Conference Paper","0"
"90","2-s2.0-85061161372","10.1007/978-3-030-10997-4_43",NA,"Du J.|Blake D.|Wang L.|Conran C.|Mckibben D.|Way A.","Dublin City University","Ireland","2019","Idea: An interactive dialogue translation demo system using furhat robots","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11053 LNAI",NA,"645-648","Dialogue translation|Entity translation|Furhat robot","© 2019, Springer Nature Switzerland AG. We showcase IDEA, an Interactive DialoguE trAnslation system using Furhat robots, whose novel contributions are: (i) it is a web service-based application combining translation service, speech recognition service and speech synthesis service; (ii) it is a task-oriented hybrid machine translation system combining statistical and neural machine learning methods for domain-specific named entity (NE) recognition and translation; and (iii) it provides user-friendly interactive interface using Furhat robot with speech input, output, head movement and facial emotions. IDEA is a case-study demo which can efficiently and accurately assist customers and agents in different languages to reach an agreement in a dialogue for the hotel booking.","Conference Paper","0"
"91","2-s2.0-85061174316","10.1109/ACCESS.2019.2891668",NA,"Deng J.|Pang G.|Zhang Z.|Pang Z.|Yang H.|Yang G.","Zhejiang University|ABB Corporate Research, Vasteras","China|Sweden","2019","CGAN Based Facial Expression Recognition for Human-Robot Interaction","IEEE Access","7",NA,"9848-9859","conditional generative adversarial network|emotion recognition|Facial expression recognition|human-robot interaction","© 2013 IEEE. As an emerging research topic for proximity service (ProSe), automatic emotion recognition enables the machines to understand the emotional changes of human beings which can not only facilitate natural, effective, seamless, and advanced human-robot interaction or human-computer interface but also promote emotional health. Facial expression recognition (FER) is a vital task for emotion recognition. However, significant gap between human and machine exists in FER task. In this paper, we present a conditional generative adversarial network-based approach to alleviate the intra-class variations by individually controlling the facial expressions and learning the generative and discriminative representations simultaneously. The proposed framework consists of a generator G and three discriminators (Di, Da, and Dexp). The generator G transforms any query face image into another prototypic facial expression image with other factors preserved. Armed with action units condition, the generator G pays more attention to information relevant to facial expression. Three loss functions (LI, La, and Lexp) corresponding to the three discriminators (Di, Da, and Dexp) were designed to learn generative and discriminative representations. Moreover, after rendering the generated expression back to its original facial expression, cycle consistency loss is also applied to guarantee the identity and produce more constrained visual representations. Optimized by combining both synthesis and classification loss functions, the learnt representation is explicitly disentangled from other variations such as identity, head pose, and illumination. Qualitative and quantitative experimental results demonstrate the proposed FER system is effective for expression recognition.","Article","0"
"92","2-s2.0-85061213841","10.1021/acs.accounts.8b00516","30698006","Qiu Y.|Zhang E.|Plamthottam R.|Pei Q.","University of California, Los Angeles","United States","2019","Dielectric Elastomer Artificial Muscle: Materials Innovations and Device Explorations","Accounts of Chemical Research",NA,NA,"",NA,"Copyright © 2019 American Chemical Society. ConspectusCreating an artificial muscle has been one of the grand challenges of science and engineering. The invention of such a flexible, versatile, and power efficient actuator opens the gate for a new generation of lightweight, highly efficient, and multifunctional robotics. Many current artificial muscle technologies enable low-power mobile actuators, robots that mimic efficient and natural forms of motion, autonomous robots and sensors, and lightweight wearable technologies. They also have serious applications in biomedical devices, where biocompatibility, from a chemical, flexibility, and force perspective, is crucial. It remains unknown which material will ultimately form the ideal artificial muscle. Anything from shape memory alloys (SMAs) to pneumatics to electroactive polymers (EAPs) realize core aspects of the artificial muscle goal. Among them, EAPs most resemble their biological counterparts, and they encompass both ion-infusion and electric field based actuation mechanisms. Some of the most investigated EAPs are dielectric elastomers (DEs), whose large strains, fracture toughness, and power-to-weight ratios compare favorably with natural muscle.Although dielectric elastomer actuators (DEAs) only entered the artificial muscle conversation in the last 20 years, significant technological progress has reflected their high potential. Research has focused on solving the core issues surrounding DEAs, which includes improving their operational ranges with regard to temperature and voltage, adding new functionality to the materials, and improving the reliability of the components on which they depend. Mechanisms designed to utilize their large-strain actuation and low stiffness has also attracted attention. This Account covers important research by our group and others in various avenues such as decreasing viscoelastic losses in typical DE materials, increasing their dielectric constant, and countering electromechanical instability. We also discuss variable stiffness polymers, specifically bistable electroactive polymers, which, notably, open DEAs to structural applications typically unattainable for soft-actuator technologies. Furthermore, we explore advancements related to highly compliant and transparent electrodes, a crucial component of DEAs capable of achieving high actuation strain. We then cover noteworthy applications, including several novel devices for soft robotics and microfluidics, and how those applications fit within other major developments in the field. Finally, we conclude with a discussion of the remaining challenges facing current DEA technology and speculate on research directions that may further advance DE-based artificial muscles as a whole. This Account serves as a stepping stone into the field of EAPs, which, through the work of researchers worldwide, are positioned as a potential challenger to conventional actuator technologies.","Article","0"
"93","2-s2.0-85061379309","10.1007/978-3-030-11548-7_36",NA,"Augello A.|Infantino I.|Maniscalco U.|Pilato G.|Vella F.","Consiglio Nazionale delle Ricerche","Italy","2019","Introducing NarRob, a robotic storyteller","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11385 LNCS",NA,"387-396","Chatbots|Social practices|Social robots|Storytelling","© 2019, Springer Nature Switzerland AG. In this work we introduce NarRob, a robot able to engage in conversations and tell stories, by accompanying the speech with proper gestures. We discuss about the main components of the robot’s architecture, and some possible education experiments that we are planning to carry out in real scholastic contexts.","Conference Paper","0"
"94","2-s2.0-85061380506","10.1007/978-3-030-04672-9_11",NA,"Fortunati L.|Cavallo F.|Sarrica M.","Università degli Studi di Udine|Scuola Superiore Sant'Anna di Studi Universitari e di Perfezionamento|Università degli Studi di Roma La Sapienza","Italy","2019","The Role of Social Robots in Public Space","Lecture Notes in Electrical Engineering","540",NA,"171-186","Public events|Public space|Social robot|Social robot audiences|Social robot role in public space","© 2019, Springer Nature Switzerland AG. The purpose of this research is to understand what might be today—on the edge of the social robotics era—the role of robots in public spaces. In Europe, there was a strong tradition of automata exhibition in the ‘600 and 700’ with ostensive purpose. Exposing automata in various fairs was meant to inspire awe towards the advancement of technology and science in the public who attended numerous public events. This study aims to investigate whether in the modern world the robot may have the same or other functions in public space. The study has analyzed the public display of the robot DORO: a technological artifact created by the Sant’Anna School during the European project ROBOT-ERA. Two distinct public moments were examined. The first was the night of researchers in Pisa on 30 September 2016 in Martiri della Libertà’s square, where DORO was exhibited to the public. The second occasion was the inauguration of the 39th academic year of the University of Udine. On this occasion, DORO brought to the rector his inaugural lecture and exchanged—in front of a large auditorium—a brief dialogue with the rector of the University of Udine, prof. Alberto de Toni.","Conference Paper","0"
"95","2-s2.0-85061393279","10.1007/978-3-030-04672-9_9",NA,"Fiorini L.|D’Onofrio G.|Limosani R.|Sancarlo D.|Greco A.|Giuliani F.|Kung A.|Dario P.|Cavallo F.","Scuola Superiore Sant'Anna di Studi Universitari e di Perfezionamento|IRCCS Casa Sollievo della Sofferenza|Trialog","Italy|France","2019","ACCRA Project: Agile Co-Creation for Robots and Aging","Lecture Notes in Electrical Engineering","540",NA,"133-150","Active aging|Agile programming|Co-creation|Service robotics","© 2019, Springer Nature Switzerland AG. The mission of ACCRA (Agile Co-Creation for Robots and Aging) is to enable the development of advanced ICT Robotics based solutions for extending active and healthy aging in daily life by defining, developing and demonstrating an agile co-creation development process. ACCRA project consists of three robotic applications which aim to promote the independent living by means of personal mobility application, to support the daily life management thanks to housework application and to promote conversation rehabilitation tailored on personal attitude by means of dedicated software programme. Additionally, ACCRA project will be designed and developed on open source framework (i.e. ROS, FIWARE, universAAL and Rospex) to promote the interoperability among scientific community.","Conference Paper","0"
"96","2-s2.0-85061726699","10.1007/978-3-030-11015-4_17",NA,"Duarte N.F.|Rakovic M.|Marques J.|Santos-Victor J.","Instituto Superior Técnico|University of Novi Sad","Portugal|Serbia","2019","Action alignment from gaze cues in human-human and human-robot interaction","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11131 LNCS",NA,"197-212","Action alignment|Action anticipation|Gaze behavior|Human-robot interaction","© Springer Nature Switzerland AG 2019. Cognitive neuroscience experiments show how people intensify the exchange of non-verbal cues when they work on a joint task towards a common goal. When individuals share their intentions, it creates a social interaction that drives the mutual alignment of their actions and behavior. To understand the intentions of others, we strongly rely on the gaze cues. According to the role each person plays in the interaction, the resulting alignment of the body and gaze movements will be different. This mechanism is key to understand and model dyadic social interactions. We focus on the alignment of the leader’s behavior during dyadic interactions. The recorded gaze movements of dyads are used to build a model of the leader’s gaze behavior. We use of the follower’s gaze behavior data for two purposes: (i) to determine whether the follower is involved in the interaction, and (ii) if the follower’s gaze behavior correlates to the type of the action under execution. This information is then used to plan the leader’s actions in order to sustain the leader/follower alignment in the social interaction. The model of the leader’s gaze behavior and the alignment of the intentions is evaluated in a human-robot interaction scenario, with the robot acting as a leader and the human as a follower. During the interaction, the robot (i) emits non-verbal cues consistent with the action performed; (ii) predicts the human actions, and (iii) aligns its motion according to the human behavior.","Conference Paper","0"
"97","2-s2.0-85061742943",NA,NA,"Prema R.|Anuradha K.|Sakthi Sabreesh K.|Sree Valsan R.","Karpagam Academy of Higher Education|Karpagam College of Engineering","India","2019","Arduino based semi-autonomous vision biped robot","International Journal of Innovative Technology and Exploring Engineering","8","4S","458-460","Arduino|Detection and recognition|MatLab|Semi-autonomous biped robot","© BEIESP. Robots are extensively used in Research and Development. Agriculture and Home automation systems play a vital role in this autonomous world. By interfacing image and voice sensor a semi – autonomous biped robot using MATLAB and ARDUINO is developed. The main objective of this robotic system is to harvest the vegetables from the farm using image sensor. This is done by detecting and comparing predefined images with live video snap shot with in limited time. Another feature of this robot is to pick and place the objects in home. This robot can be used for various domestic purposes.","Article","0"
